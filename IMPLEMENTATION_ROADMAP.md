# AgriTool Enhanced Scraper Implementation Roadmap

## âœ… Phase 0: Preparation (COMPLETED)
- [x] Organized legacy code in `/legacy/` folder
- [x] Cleaned up TypeScript errors
- [x] Prepared repository secrets:
  - `DB_GITHUB_SCRAPER`
  - `LOVABLE_REGULINE` 
  - `NEXT_PUBLIC_SUPABASE_ANON_KEY`
  - `NEXT_PUBLIC_SUPABASE_URL`
  - `OPENAI_API_KEY`
  - `SCRAPER_RAW_GPT_API`
  - `SCRAPPER_RAW_GPT_API` (legacy typo version)
  - `SUPABASE_SERVICE_ROLE_KEY`

## ðŸš€ Phase 1: Enhanced Scraper Foundation (Week 1-2)

### Task 1A: Universal Content Harvester Core (Days 1-3)
**Priority**: HIGH
**Description**: Create the foundation for verbatim content extraction with format preservation

**Components to build**:
- `src/scraper/core/UniversalHarvester.ts` - Main scraper class
- `src/scraper/core/ContentPreserver.ts` - Format preservation system
- `src/scraper/core/DocumentExtractor.ts` - Enhanced document processing
- `supabase/functions/scraper-orchestrator/` - Edge function coordinator

**Success Criteria**:
- âœ… Extract content with exact HTML structure preservation
- âœ… Maintain table relationships and CSS styling metadata
- âœ… Process PDFs with layout preservation
- âœ… Store raw content with full formatting data

### Task 1B: Multi-Source Discovery Engine (Days 4-5)
**Priority**: HIGH
**Description**: Build intelligent URL discovery with sitemap crawling

**Components to build**:
- `src/scraper/discovery/SitemapCrawler.ts`
- `src/scraper/discovery/PaginationHandler.ts`
- `src/scraper/discovery/ContentValidator.ts`

**Success Criteria**:
- âœ… Discover URLs from sitemaps and RSS feeds
- âœ… Handle dynamic pagination and infinite scroll
- âœ… Validate content freshness and changes

### Task 1C: Deployment Pipeline Enhancement (Days 6-7)
**Priority**: MEDIUM
**Description**: Implement hybrid GitHub Actions + Supabase approach

**Components to build**:
- `.github/workflows/enhanced-scraper-pipeline.yml`
- `supabase/functions/real-time-validator/`
- Update existing config in `supabase/config.toml`

**Success Criteria**:
- âœ… Daily automated scraping via GitHub Actions
- âœ… Real-time validation via Supabase Edge Functions
- âœ… Error handling and retry mechanisms

---

## ðŸ“‹ Next Phases Preview

### Phase 2: AI Canonicalization Enhancement (Week 3-4)
- Enhanced AI processing with country-specific prompts
- Multi-pass validation and cross-reference checking
- Canonical data structure implementation

### Phase 3: Advanced Form Generation (Week 5-6) 
- PDF form field extraction and analysis
- Dynamic questionnaire creation
- Smart form validation and prefilling

### Phase 4: UI/UX Innovation (Week 7-8)
- Dual-view interfaces (original + structured)
- Interactive document viewing
- Enhanced application workflows

### Phase 5: System Integration (Week 9-10)
- Performance optimization
- Quality assurance automation
- Production deployment

---

## ðŸŽ¯ Immediate Next Steps

**Ready to start with Task 1A**: Universal Content Harvester Core

This will establish the foundation for the entire enhanced scraper system while maintaining compatibility with existing functionality.

**Estimated effort**: 3 days
**Risk level**: Low (additive changes only)
**Dependencies**: Repository secrets (already configured)