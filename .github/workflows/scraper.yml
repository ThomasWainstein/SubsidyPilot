
name: AgriTool Scraper Pipeline

on:
  schedule:
    # Run daily at 6 AM UTC (7 AM CET)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      target_url:
        description: 'Target URL to scrape (optional, defaults to AFIR)'
        required: false
        default: 'https://www.afir.info/'
      max_pages:
        description: 'Maximum pages to scrape (default: 0 = all)'
        required: false
        default: '0'
      dry_run:
        description: 'Dry run mode (scrape but do not upload to Supabase)'
        required: false
        type: boolean
        default: false

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          chromium-browser \
          xvfb \
          file
          
    - name: Install Python dependencies
      run: |
        cd AgriToolScraper-main
        pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create data directories
      run: |
        mkdir -p AgriToolScraper-main/data/extracted
        mkdir -p AgriToolScraper-main/data/logs
        
        
    - name: Debug system Chrome installation
      run: |
        echo "🔍 Debugging Chrome/Chromium installation..."
        which chromium-browser || echo "chromium-browser not found"
        chromium-browser --version || echo "Cannot get chromium version"
        
    - name: Setup virtual display
      env:
        DISPLAY: :99
      run: |
        Xvfb :99 -screen 0 1920x1080x24 &
        sleep 3
        
        
    - name: Run scraper pipeline with Ruthless Debugging
      env:
        DB_GITHUB_SCRAPER: ${{ secrets.DB_GITHUB_SCRAPER }}
        NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
        NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        TARGET_URL: ${{ github.event.inputs.target_url || 'https://www.afir.info/' }}
        MAX_PAGES: ${{ github.event.inputs.max_pages || '0' }}
        DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
        BROWSER: chrome
        DISPLAY: :99
        WDM_LOG: "1"
        WDM_LOG_LEVEL: "DEBUG"
      run: |
        set -e
        cd AgriToolScraper-main
        
        echo "🔥 RUTHLESS DEBUGGING ENABLED"
        echo "=== SYSTEM DIAGNOSTICS ==="
        echo "Environment: $(uname -a)"
        echo "Python: $(python --version)"
        echo "Working dir: $(pwd)"
        echo "Display: $DISPLAY"
        echo "User: $(whoami)"
        echo "Home: $HOME"
        
        echo "=== BROWSER DIAGNOSTICS ==="
        chromium-browser --version || echo "Chromium not found"
        which chromium-browser || echo "Chromium not in PATH"
        ls -la /usr/bin/chromium* || echo "No chromium binaries found"
        
        echo "=== DISPLAY DIAGNOSTICS ==="
        xdpyinfo || echo "Display test failed"
        echo "DISPLAY=$DISPLAY"
        
        echo "=== PYTHON ENVIRONMENT ==="
        pip freeze | head -20
        
        # Create all necessary directories
        mkdir -p data/logs data/extracted data/attachments
        
        echo "🚀 Starting AgriTool scraper pipeline with full logging..."
        python main.py || {
          echo "❌ SCRAPER FAILED - Collecting emergency diagnostics"
          echo "=== POST-MORTEM DIAGNOSTICS ==="
          echo "Process list:"
          ps aux | grep -E "(chrome|chromium|firefox|driver|xvfb)" || true
          echo "Memory usage:"
          free -h || true
          echo "Disk usage:"
          df -h || true
          echo "Open files:"
          lsof 2>/dev/null | head -50 || true
          echo "webdriver-manager cache:"
          find ~/.wdm -type f -exec file {} \; 2>/dev/null || true
          exit 1
        }
        
    - name: Upload comprehensive debugging artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-complete-debug-${{ github.run_number }}
        path: |
          AgriToolScraper-main/data/logs/
          AgriToolScraper-main/data/extracted/
          AgriToolScraper-main/data/attachments/
          ~/.wdm/
        retention-days: 14
        
    - name: Upload critical status files
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-status-${{ github.run_number }}
        path: |
          AgriToolScraper-main/data/logs/status_*.txt
          AgriToolScraper-main/data/logs/summary_*.txt
          AgriToolScraper-main/data/logs/diagnostics_*.json
          AgriToolScraper-main/data/logs/run_summary.json
        retention-days: 30

    - name: Upload pip freeze for debugging
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: python-environment-${{ github.run_number }}
        path: |
          AgriToolScraper-main/data/logs/pip_freeze_*.txt
        retention-days: 7
        
