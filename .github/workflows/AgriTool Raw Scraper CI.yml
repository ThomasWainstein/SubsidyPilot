# =============================================================================
# AgriTool Raw Scraper CI - ROBUST WORKFLOW
# =============================================================================
# This workflow implements comprehensive robustness requirements:
# - System Chrome & Chromedriver installation (no exec format errors)
# - Virtual display setup (Xvfb) for headless Selenium
# - Binary validation and debug logging
# - Secure secret management for Supabase
# - Comprehensive artifact uploads
# - Failure handling with detailed logs
# - Manual and scheduled triggers
# - Retry logic and error handling
# =============================================================================

name: 🔥 AgriTool Raw Scraper CI - BULLETPROOF

on:
  push:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
      - '.github/workflows/AgriTool Raw Scraper CI.yml'
  pull_request:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
  workflow_dispatch:
    inputs:
      site:
        description: 'Target site to scrape'
        required: false
        default: 'franceagrimer'
        type: choice
        options:
          - franceagrimer
          - all
      start_page:
        description: 'Start page number'
        required: false
        default: '0'
      end_page:
        description: 'End page number'
        required: false
        default: '2'
      dry_run:
        description: 'Dry run upload (no actual Supabase insert)'
        required: false
        default: false
        type: boolean
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'

env:
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'
  
jobs:
  bulletproof-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    defaults:
      run:
        working-directory: AI_SCRAPER_RAW_TEXTS
    steps:
      - name: 📋 Checkout code
        uses: actions/checkout@v4

      - name: 🐍 Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: 🔧 Install system dependencies (Chrome/Chromedriver/Xvfb)
        run: |
          echo "🔧 Installing system dependencies..."
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser chromium-chromedriver xvfb file wget curl
          echo "✅ System dependencies installed"

      - name: 🖥️ Setup virtual display (Xvfb)
        run: |
          echo "🖥️ Starting virtual display..."
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          export DISPLAY=:99
          sleep 3
          echo "✅ Virtual display started on :99"
        env:
          DISPLAY: ':99'

      - name: 🔍 Debug Chrome/Chromedriver installation
        run: |
          echo "🔍 === CHROME/CHROMEDRIVER DEBUG INFO ==="
          echo "📍 Chromium browser location:"
          which chromium-browser || echo "❌ chromium-browser not found"
          if command -v chromium-browser >/dev/null 2>&1; then
            chromium-browser --version || echo "❌ Cannot get Chromium version"
          fi
          
          echo "📍 Chromedriver location:"
          which chromedriver || echo "❌ chromedriver not found"
          if command -v chromedriver >/dev/null 2>&1; then
            chromedriver --version || echo "❌ Cannot get Chromedriver version"
          fi
          
          echo "📍 Binary details:"
          ls -l /usr/bin/chromedriver || echo "❌ /usr/bin/chromedriver not found"
          ls -l /usr/bin/chromium-browser || echo "❌ /usr/bin/chromium-browser not found"
          
          echo "📍 Display info:"
          echo "DISPLAY: $DISPLAY"
          ps aux | grep Xvfb || echo "❌ Xvfb not running"
          echo "✅ Debug info complete"

      - name: 🧹 Clear webdriver-manager cache
        run: |
          echo "🧹 Clearing webdriver-manager cache..."
          rm -rf ~/.wdm || true
          rm -rf /home/runner/.wdm || true
          echo "✅ Webdriver-manager cache cleared"

      - name: 📦 Install Python dependencies
        run: |
          echo "📦 Installing Python dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "✅ Python dependencies installed"
          echo "📋 Installed packages:"
          pip list | grep -E "(selenium|supabase|requests|beautifulsoup4)"

      - name: 🔍 Validate environment variables
        run: |
          echo "🔍 Validating required Supabase environment variables..."
          python ../test_env_vars.py
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: 🧪 Test system chromedriver directly
        run: |
          echo "🧪 Testing system chromedriver..."
          chromedriver --version
          python -c "
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          options = Options()
          options.add_argument('--headless=new')
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          driver = webdriver.Chrome(service=Service('/usr/bin/chromedriver'), options=options)
          print('✅ System chromedriver test successful')
          driver.quit()
          "
          echo "✅ System chromedriver validation completed"

      - name: 🧪 Run comprehensive test suite
        run: |
          echo "🧪 Running test suite..."
          python test_scraper.py
          echo "✅ Test suite completed successfully"

      - name: 🕷️ Batch scrape (FranceAgriMer pages ${{ github.event.inputs.start_page || '0' }}–${{ github.event.inputs.end_page || '2' }})
        run: |
          echo "🕷️ Starting batch scrape..."
          SITE="${{ github.event.inputs.site || 'franceagrimer' }}"
          START_PAGE="${{ github.event.inputs.start_page || '0' }}"
          END_PAGE="${{ github.event.inputs.end_page || '2' }}"
          
          echo "📊 Scrape parameters:"
          echo "  Site: $SITE"
          echo "  Start page: $START_PAGE"
          echo "  End page: $END_PAGE"
          
          python scraper_main.py --site $SITE --start-page $START_PAGE --end-page $END_PAGE
          echo "✅ Batch scrape completed"

      - name: 📊 Validate scraper outputs
        run: |
          echo "📊 === SCRAPER OUTPUT VALIDATION ==="
          echo "📁 Raw pages directory:"
          if [ -d "data/raw_pages" ]; then
            ls -la data/raw_pages/ | head -20
            echo "📋 Total JSON files: $(find data/raw_pages -name "*.json" | wc -l)"
          else
            echo "❌ data/raw_pages directory not found"
          fi
          
          echo "📁 Attachments directory:"
          if [ -d "data/attachments" ]; then
            ls -la data/attachments/ | head -10
            echo "📋 Total attachments: $(find data/attachments -type f | wc -l)"
          else
            echo "⚠️ data/attachments directory not found (may be empty)"
          fi
          
          echo "📁 Logs directory:"
          if [ -d "data/logs" ]; then
            ls -la data/logs/
          else
            echo "⚠️ data/logs directory not found"
          fi

      - name: 🔄 Upload raw data to Supabase (dry run validation)
        run: |
          echo "🔄 Running Supabase upload dry run..."
          python upload_raw_to_supabase.py --dry-run --batch-size 25
          echo "✅ Dry run validation completed"

      - name: 🚀 Upload raw data to Supabase (live upload)
        if: ${{ github.event.inputs.dry_run != 'true' }}
        run: |
          echo "🚀 Starting live Supabase upload..."
          python upload_raw_to_supabase.py --batch-size 25
          echo "✅ Live upload completed"
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        continue-on-error: false

      - name: 📋 Upload success validation
        run: |
          echo "📋 === UPLOAD SUCCESS VALIDATION ==="
          if [ -f "data/logs/supabase_upload.log" ]; then
            echo "📄 Upload log found - showing last 30 lines:"
            tail -30 data/logs/supabase_upload.log
            
            echo "📊 Upload statistics:"
            grep -E "(Uploaded|Error|Skip)" data/logs/supabase_upload.log | tail -10 || true
          else
            echo "⚠️ Upload log not found at data/logs/supabase_upload.log"
          fi

      - name: 📦 Upload raw_pages as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: raw_pages-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/raw_pages/
          retention-days: 30

      - name: 📦 Upload attachments as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: attachments-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/attachments/
          retention-days: 30

      - name: 📦 Upload logs as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: logs-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/logs/
          retention-days: 7

      - name: 🎯 Success Summary
        if: success()
        run: |
          echo "🎯 === WORKFLOW SUCCESS SUMMARY ==="
          echo "✅ All steps completed successfully"
          echo "✅ Chrome/Chromedriver: Working"
          echo "✅ Virtual display: Working"
          echo "✅ Test suite: Passed"
          echo "✅ Scraping: Completed"
          echo "✅ Supabase upload: Success"
          echo "✅ Artifacts: Uploaded"
          echo ""
          echo "📊 Final statistics:"
          echo "  JSON files: $(find data/raw_pages -name "*.json" 2>/dev/null | wc -l)"
          echo "  Attachments: $(find data/attachments -type f 2>/dev/null | wc -l)"
          echo "  Run ID: ${{ github.run_number }}"

      - name: ❌ Failure Debug & Logs
        if: failure()
        run: |
          echo "❌ === WORKFLOW FAILURE DEBUG ==="
          echo "❌ Workflow failed - collecting debug information"
          echo ""
          echo "🔍 Environment info:"
          echo "  Python version: $(python --version)"
          echo "  DISPLAY: $DISPLAY"
          echo "  Working directory: $(pwd)"
          echo ""
          echo "🔍 Process info:"
          ps aux | grep -E "(Xvfb|chrome)" || echo "No Chrome/Xvfb processes found"
          echo ""
          echo "🔍 Directory structure:"
          find . -type d -name "data" -exec ls -la {} \; 2>/dev/null || true
          echo ""
          echo "🔍 Recent logs (last 50 lines):"
          find data/logs -name "*.log" -exec echo "=== {} ===" \; -exec tail -50 {} \; 2>/dev/null || echo "No log files found"
          echo ""
          echo "🔍 Chrome/driver debug:"
          which chromium-browser chromedriver || true
          ls -la /usr/bin/chrome* 2>/dev/null || true
          echo ""
          echo "❌ Debug information collection complete"
