# =============================================================================
# AgriTool Raw Scraper CI - BULLETPROOF (FIXED)
# =============================================================================
# Fixes applied:
# - File name/path: no spaces, matches triggers
# - All required secrets set and passed to relevant steps (including GPT API key)
# - Clearer environment variable validation and error output
# - Robust defaults for inputs
# - Workflow_dispatch input logic clarified
# =============================================================================

name: üî• AgriTool Raw Scraper CI - BULLETPROOF

on:
  push:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
      - '.github/workflows/agritool-raw-scraper-ci.yml'
  pull_request:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
  workflow_dispatch:
    inputs:
      site:
        description: 'Target site to scrape'
        required: false
        default: 'franceagrimer'
        type: choice
        options:
          - franceagrimer
          - all
      start_page:
        description: 'Start page number'
        required: false
        default: '0'
      end_page:
        description: 'End page number (use -1 to scrape all pages)'
        required: false
        default: '2'
      max_pages:
        description: 'Maximum number of pages to scrape (optional limit)'
        required: false
        default: ''
      max_urls:
        description: 'Maximum number of URLs to collect (optional limit)'
        required: false
        default: ''
      scrape_all:
        description: 'Scrape all pages until no more results'
        required: false
        default: false
        type: boolean
      dry_run:
        description: 'Dry run upload (no actual Supabase insert)'
        required: false
        default: false
        type: boolean
  schedule:
    - cron: '0 6 * * *'

env:
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'

jobs:
  bulletproof-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    defaults:
      run:
        working-directory: AI_SCRAPER_RAW_TEXTS
    steps:
      - name: üìã Checkout code
        uses: actions/checkout@v4

      - name: üêç Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: üîß Install system dependencies (Chrome/Chromedriver/Xvfb)
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser chromium-chromedriver xvfb file wget curl jq

      - name: üñ•Ô∏è Setup virtual display (Xvfb)
        run: |
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          export DISPLAY=:99
          sleep 3
        env:
          DISPLAY: ':99'

      - name: üîç Debug Chrome/Chromedriver installation
        run: |
          which chromium-browser || echo "‚ùå chromium-browser not found"
          if command -v chromium-browser >/dev/null 2>&1; then chromium-browser --version; fi
          which chromedriver || echo "‚ùå chromedriver not found"
          if command -v chromedriver >/dev/null 2>&1; then chromedriver --version; fi
          ls -l /usr/bin/chromedriver || true
          ls -l /usr/bin/chromium-browser || true
          echo "DISPLAY: $DISPLAY"
          ps aux | grep Xvfb || true

      - name: üßπ Clear webdriver-manager cache
        run: |
          rm -rf ~/.wdm || true
          rm -rf /home/runner/.wdm || true

      - name: üì¶ Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip list | grep -E "(selenium|supabase|requests|beautifulsoup4)"

      - name: üîç Validate environment variables
        run: |
          python ../test_env_vars.py
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}

      - name: üß™ Test system chromedriver directly
        run: |
          chromedriver --version
          python -c "
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          options = Options()
          options.add_argument('--headless=new')
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          driver = webdriver.Chrome(service=Service('/usr/bin/chromedriver'), options=options)
          print('‚úÖ System chromedriver test successful')
          driver.quit()
          "

      - name: üß™ Run comprehensive test suite
        run: |
          python test_scraper.py

      - name: üï∑Ô∏è Batch scrape with flexible pagination
        run: |
          # Use workflow_dispatch inputs ONLY if manually triggered, otherwise use defaults
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            SITE="${{ github.event.inputs.site }}"
            START_PAGE="${{ github.event.inputs.start_page }}"
            END_PAGE="${{ github.event.inputs.end_page }}"
            MAX_PAGES="${{ github.event.inputs.max_pages }}"
            MAX_URLS="${{ github.event.inputs.max_urls }}"
            SCRAPE_ALL="${{ github.event.inputs.scrape_all }}"
            DRY_RUN="${{ github.event.inputs.dry_run }}"
          else
            SITE="franceagrimer"
            START_PAGE="0"
            END_PAGE="2"
            MAX_PAGES=""
            MAX_URLS=""
            SCRAPE_ALL="false"
            DRY_RUN="false"
          fi

          CMD="python scraper_main.py --site $SITE --start-page $START_PAGE --end-page $END_PAGE"
          if [ "$SCRAPE_ALL" = "true" ]; then
            CMD="$CMD --scrape-all"
          fi
          if [ -n "$MAX_PAGES" ]; then
            CMD="$CMD --max-pages $MAX_PAGES"
          fi
          if [ -n "$MAX_URLS" ]; then
            CMD="$CMD --max-urls $MAX_URLS"
          fi

          echo "üöÄ Executing: $CMD"
          eval $CMD

        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}

      - name: üìä Validate scraper outputs
        run: |
          echo "üìÅ Raw pages directory:"
          if [ -d "data/raw_pages" ]; then
            ls -la data/raw_pages/ | head -20
            echo "üìã Total JSON files: $(find data/raw_pages -name "*.json" | wc -l)"
          else
            echo "‚ùå data/raw_pages directory not found"
          fi
          echo "üìÅ Attachments directory:"
          if [ -d "data/attachments" ]; then
            ls -la data/attachments/ | head -10
            echo "üìã Total attachments: $(find data/attachments -type f | wc -l)"
          else
            echo "‚ö†Ô∏è data/attachments directory not found (may be empty)"
          fi
          echo "üìÅ Logs directory:"
          if [ -d "data/logs" ]; then
            ls -la data/logs/
          else
            echo "‚ö†Ô∏è data/logs directory not found"
          fi

      - name: üîÑ Upload raw data to Supabase (dry run validation)
        run: |
          python upload_raw_to_supabase.py --dry-run --batch-size 25
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}

      - name: üöÄ Upload raw data to Supabase (live upload)
        if: ${{ github.event.inputs.dry_run != 'true' }}
        run: |
          python upload_raw_to_supabase.py --batch-size 25
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}

      - name: üìã Upload success validation
        run: |
          if [ -f "data/logs/supabase_upload.log" ]; then
            tail -30 data/logs/supabase_upload.log
            grep -E "(Uploaded|Error|Skip)" data/logs/supabase_upload.log | tail -10 || true
          else
            echo "‚ö†Ô∏è Upload log not found at data/logs/supabase_upload.log"
          fi

      - name: üì¶ Upload raw_pages as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: raw_pages-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/raw_pages/
          retention-days: 30

      - name: üì¶ Upload attachments as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: attachments-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/attachments/
          retention-days: 30

      - name: üì¶ Upload logs as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: logs-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/logs/
          retention-days: 7

      - name: üéØ Success Summary
        if: success()
        run: |
          echo "üéØ === WORKFLOW SUCCESS SUMMARY ==="
          echo "‚úÖ All steps completed successfully"
          echo "‚úÖ Chrome/Chromedriver: Working"
          echo "‚úÖ Virtual display: Working"
          echo "‚úÖ Test suite: Passed"
          echo "‚úÖ Scraping: Completed"
          echo "‚úÖ Supabase upload: Success"
          echo "‚úÖ Artifacts: Uploaded"
          echo ""
          echo "üìä Final statistics:"
          echo "  JSON files: $(find data/raw_pages -name "*.json" 2>/dev/null | wc -l)"
          echo "  Attachments: $(find data/attachments -type f 2>/dev/null | wc -l)"
          echo "  Run ID: ${{ github.run_number }}"

      - name: ‚ùå Failure Debug & Logs
        if: failure()
        run: |
          echo "‚ùå === WORKFLOW FAILURE DEBUG ==="
          echo "‚ùå Workflow failed - collecting debug information"
          echo ""
          echo "üîç Environment info:"
          echo "  Python version: $(python --version)"
          echo "  DISPLAY: $DISPLAY"
          echo "  Working directory: $(pwd)"
          echo ""
          echo "üîç Process info:"
          ps aux | grep -E "(Xvfb|chrome)" || echo "No Chrome/Xvfb processes found"
          echo ""
          echo "üîç Directory structure:"
          find . -type d -name "data" -exec ls -la {} \; 2>/dev/null || true
          echo ""
          echo "üîç Recent logs (last 50 lines):"
          find data/logs -name "*.log" -exec echo "=== {} ===" \; -exec tail -50 {} \; 2>/dev/null || echo "No log files found"
          echo ""
          echo "üîç Chrome/driver debug:"
          which chromium-browser chromedriver || true
          ls -la /usr/bin/chrome* 2>/dev/null || true
          echo ""
          echo "‚ùå Debug information collection complete"
