# =============================================================================
# AgriTool Raw Scraper CI - IMPROVED WORKFLOW
# =============================================================================
# Improvements made:
# - Simplified pagination: max_pages instead of start_page/end_page
# - Auto-scrape when max_pages=0 (scrapes all available pages)
# - Enhanced progress tracking with individual URL reporting
# - Clear statistics: pages scraped, URLs processed, success rates
# - Professional summary reports with detailed metrics
# - Better error handling with URL-level failure tracking
# =============================================================================

name: ğŸ”¥ AgriTool Raw Scraper CI - SMART & CLEAR

on:
  push:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
      - '.github/workflows/agritool-raw-scraper-ci.yml'
  pull_request:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
  workflow_dispatch:
    inputs:
      site:
        description: 'Target site to scrape'
        required: false
        default: 'franceagrimer'
        type: choice
        options:
          - franceagrimer
          - all
      max_pages:
        description: 'Number of pages to scrape (0 = scrape all available pages)'
        required: false
        default: '5'
        type: string
      max_urls_per_page:
        description: 'Maximum URLs to collect per page (0 = no limit)'
        required: false
        default: '0'
        type: string
      dry_run:
        description: 'Dry run upload (no actual Supabase insert)'
        required: false
        default: false
        type: boolean
  schedule:
    - cron: '0 6 * * *'

env:
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'
  
jobs:
  smart-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    defaults:
      run:
        working-directory: AI_SCRAPER_RAW_TEXTS
    steps:
      - name: ğŸ“‹ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: ğŸ”§ Install system dependencies (Chrome/Chromedriver/Xvfb)
        run: |
          echo "ğŸ”§ Installing system dependencies..."
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser chromium-chromedriver xvfb file wget curl jq bc
          echo "âœ… System dependencies installed"

      - name: ğŸ–¥ï¸ Setup virtual display (Xvfb)
        run: |
          echo "ğŸ–¥ï¸ Starting virtual display..."
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          export DISPLAY=:99
          sleep 3
          echo "âœ… Virtual display started on :99"
        env:
          DISPLAY: ':99'

      - name: ğŸ” Debug Chrome/Chromedriver installation
        run: |
          echo "ğŸ” === CHROME/CHROMEDRIVER DEBUG INFO ==="
          echo "ğŸ“ Chromium browser location:"
          which chromium-browser || echo "âŒ chromium-browser not found"
          if command -v chromium-browser >/dev/null 2>&1; then
            chromium-browser --version || echo "âŒ Cannot get Chromium version"
          fi
          
          echo "ğŸ“ Chromedriver location:"
          which chromedriver || echo "âŒ chromedriver not found"
          if command -v chromedriver >/dev/null 2>&1; then
            chromedriver --version || echo "âŒ Cannot get Chromedriver version"
          fi
          
          echo "ğŸ“ Display info:"
          echo "DISPLAY: $DISPLAY"
          ps aux | grep Xvfb || echo "âŒ Xvfb not running"
          echo "âœ… Debug info complete"

      - name: ğŸ§¹ Clear webdriver-manager cache
        run: |
          echo "ğŸ§¹ Clearing webdriver-manager cache..."
          rm -rf ~/.wdm || true
          rm -rf /home/runner/.wdm || true
          echo "âœ… Webdriver-manager cache cleared"

      - name: ğŸ“¦ Install Python dependencies
        run: |
          echo "ğŸ“¦ Installing Python dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "âœ… Python dependencies installed"
          echo "ğŸ“‹ Installed packages:"
          pip list | grep -E "(selenium|supabase|requests|beautifulsoup4)"

      - name: ğŸ” Validate environment variables
        run: |
          echo "ğŸ” Validating required Supabase environment variables..."
          if [ -f "../test_env_vars.py" ]; then
            python ../test_env_vars.py
          else
            echo "âš ï¸ test_env_vars.py not found, skipping validation"
          fi
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: ğŸ§ª Test system chromedriver directly
        run: |
          echo "ğŸ§ª Testing system chromedriver..."
          chromedriver --version
          python -c "
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          options = Options()
          options.add_argument('--headless=new')
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          driver = webdriver.Chrome(service=Service('/usr/bin/chromedriver'), options=options)
          print('âœ… System chromedriver test successful')
          driver.quit()
          "
          echo "âœ… System chromedriver validation completed"

      - name: ğŸ§ª Run comprehensive test suite
        run: |
          echo "ğŸ§ª Running test suite..."
          if [ -f "test_scraper.py" ]; then
            python test_scraper.py
            echo "âœ… Test suite completed successfully"
          else
            echo "âš ï¸ test_scraper.py not found, skipping test suite"
          fi

      - name: ğŸ“Š Display scraping configuration
        run: |
          echo "ğŸ“Š === SCRAPING CONFIGURATION ==="
          echo "ğŸ”„ Event type: ${{ github.event_name }}"
          
          # Handle inputs safely for all event types
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            SITE="${{ github.event.inputs.site }}"
            MAX_PAGES="${{ github.event.inputs.max_pages }}"
            MAX_URLS="${{ github.event.inputs.max_urls_per_page }}"
            DRY_RUN="${{ github.event.inputs.dry_run }}"
          else
            SITE=""
            MAX_PAGES=""
            MAX_URLS=""
            DRY_RUN=""
          fi
          
          # Apply defaults
          SITE="${SITE:-franceagrimer}"
          MAX_PAGES="${MAX_PAGES:-5}"
          MAX_URLS="${MAX_URLS:-0}"
          DRY_RUN="${DRY_RUN:-false}"
          
          echo "ğŸ¯ Target site: $SITE"
          if [ "$MAX_PAGES" = "0" ]; then
            echo "ğŸ“„ Pages to scrape: ALL available pages"
          else
            echo "ğŸ“„ Pages to scrape: $MAX_PAGES pages maximum"
          fi
          if [ "$MAX_URLS" = "0" ]; then
            echo "ğŸ”— URLs per page: no limit"
          else
            echo "ğŸ”— URLs per page: $MAX_URLS URLs maximum"
          fi
          echo "ğŸ§ª Dry run mode: $DRY_RUN"
          echo "â° Started at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          
          # Export for next steps
          echo "SCRAPER_SITE=$SITE" >> $GITHUB_ENV
          echo "SCRAPER_MAX_PAGES=$MAX_PAGES" >> $GITHUB_ENV
          echo "SCRAPER_MAX_URLS=$MAX_URLS" >> $GITHUB_ENV
          echo "SCRAPER_DRY_RUN=$DRY_RUN" >> $GITHUB_ENV

      - name: ğŸ•·ï¸ Execute smart scraping with progress tracking
        run: |
          echo "ğŸ•·ï¸ === STARTING SMART SCRAPING ==="
          SITE="$SCRAPER_SITE"
          MAX_PAGES="$SCRAPER_MAX_PAGES"
          MAX_URLS="$SCRAPER_MAX_URLS"
          
          if [ "$MAX_PAGES" = "0" ]; then
            echo "ğŸ”„ AUTO-SCRAPE MODE: Will scrape ALL available pages"
            CMD="python scraper_main.py --site $SITE --scrape-all"
          else
            echo "ğŸ“Š LIMITED SCRAPE MODE: Will scrape maximum $MAX_PAGES pages"
            CMD="python scraper_main.py --site $SITE --max-pages $MAX_PAGES"
          fi
          
          if [ "$MAX_URLS" != "0" ] && [ -n "$MAX_URLS" ]; then
            CMD="$CMD --max-urls-per-page $MAX_URLS"
            echo "ğŸ”— URL limit per page: $MAX_URLS"
          fi
          
          # Add verbose and progress flags if supported
          CMD="$CMD --verbose"
          
          echo "ğŸš€ Executing: $CMD"
          echo ""
          
          START_TIME=$(date +%s)
          if eval $CMD; then
            echo "âœ… Scraper completed successfully"
          else
            echo "âŒ Scraper failed with exit code $?"
            exit 1
          fi
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo ""
          echo "âœ… Scraping completed in ${DURATION} seconds"
          
          mkdir -p data/logs
          echo "{\"scraping_duration_seconds\": $DURATION, \"completed_at\": \"$(date -u '+%Y-%m-%d %H:%M:%S UTC')\"}" > data/logs/basic_stats.json

      - name: ğŸ“ˆ Detailed scraping results analysis
        run: |
          echo "ğŸ“ˆ === DETAILED SCRAPING RESULTS ==="
          
          JSON_FILES=0
          ATTACHMENTS=0
          
          if [ -d "data/raw_pages" ]; then
            JSON_FILES=$(find data/raw_pages -name "*.json" 2>/dev/null | wc -l)
          fi
          
          if [ -d "data/attachments" ]; then
            ATTACHMENTS=$(find data/attachments -type f 2>/dev/null | wc -l)
          fi
          
          echo "ğŸ“ Content files created:"
          echo "  ğŸ“„ JSON pages: $JSON_FILES files"
          echo "  ğŸ“ Attachments: $ATTACHMENTS files"
          
          if [ -d "data/raw_pages" ] && [ "$JSON_FILES" -gt "0" ]; then
            TOTAL_SIZE=$(du -sh data/raw_pages 2>/dev/null | cut -f1 || echo "0B")
            echo "  ğŸ’¾ Total size: $TOTAL_SIZE"
          fi
          
          echo ""
          echo "ğŸ“‹ Sample of scraped content:"
          if [ "$JSON_FILES" -gt "0" ]; then
            find data/raw_pages -name "*.json" 2>/dev/null | head -5 | while read file; do
              if [ -f "$file" ]; then
                URL=$(jq -r '.url // .source_url // "Unknown URL"' "$file" 2>/dev/null || echo "Unknown URL")
                SIZE=$(du -h "$file" 2>/dev/null | cut -f1 || echo "?")
                TITLE=$(jq -r '.title // .page_title // "No title"' "$file" 2>/dev/null | head -c 60 || echo "No title")
                echo "  âœ… $URL ($SIZE) - $TITLE"
              fi
            done
          else
            echo "  âš ï¸ No JSON files found"
          fi
          
          echo ""
          echo "ğŸ” Error analysis:"
          if [ -f "data/logs/scraper.log" ]; then
            ERROR_COUNT=$(grep -c "ERROR\|FAILED" data/logs/scraper.log 2>/dev/null || echo "0")
            WARNING_COUNT=$(grep -c "WARNING\|WARN" data/logs/scraper.log 2>/dev/null || echo "0")
            echo "  âŒ Errors found: $ERROR_COUNT"
            echo "  âš ï¸ Warnings found: $WARNING_COUNT"
            
            if [ "$ERROR_COUNT" -gt "0" ]; then
              echo "  ğŸ“„ Recent errors:"
              grep "ERROR\|FAILED" data/logs/scraper.log | tail -3 | while read line; do
                echo "    âŒ $line"
              done
            fi
          else
            echo "  â„¹ï¸ No detailed error log found"
          fi

      - name: ğŸ“Š Generate comprehensive statistics
        run: |
          echo "ğŸ“Š === COMPREHENSIVE STATISTICS ==="
          
          TOTAL_PAGES=0
          SUCCESSFUL_PAGES=0
          TOTAL_URLS=0
          SUCCESSFUL_URLS=0
          FAILED_URLS=0
          
          if [ -d "data/raw_pages" ]; then
            TOTAL_PAGES=$(find data/raw_pages -name "page_*.json" 2>/dev/null | wc -l || echo "0")
            SUCCESSFUL_PAGES=$TOTAL_PAGES
            TOTAL_URLS=$(find data/raw_pages -name "*.json" ! -name "page_*.json" 2>/dev/null | wc -l || echo "0")
            SUCCESSFUL_URLS=$TOTAL_URLS
          fi
          
          if [ "$TOTAL_URLS" -gt "0" ]; then
            SUCCESS_RATE=$(awk "BEGIN {printf \"%.1f\", $SUCCESSFUL_URLS * 100 / $TOTAL_URLS}")
          else
            SUCCESS_RATE="0.0"
          fi
          
          if [ -f "data/logs/basic_stats.json" ]; then
            DURATION=$(jq -r '.scraping_duration_seconds // 0' data/logs/basic_stats.json 2>/dev/null || echo "0")
            DURATION_MIN=$((DURATION / 60))
            DURATION_SEC=$((DURATION % 60))
          else
            DURATION_MIN=0
            DURATION_SEC=0
          fi
          
          ATTACHMENTS_COUNT=0
          if [ -d "data/attachments" ]; then
            ATTACHMENTS_COUNT=$(find data/attachments -type f 2>/dev/null | wc -l || echo "0")
          fi
          
          DATA_SIZE="0B"
          if [ -d "data" ]; then
            DATA_SIZE=$(du -sh data/ 2>/dev/null | cut -f1 || echo "0B")
          fi
          
          echo "ğŸ¯ === FINAL SCRAPING REPORT ==="
          echo "âœ… Pages processed: $SUCCESSFUL_PAGES pages"
          echo "âœ… URLs scraped: $SUCCESSFUL_URLS total URLs"
          echo "âœ… Success rate: ${SUCCESS_RATE}% ($SUCCESSFUL_URLS/$TOTAL_URLS URLs)"
          if [ "$FAILED_URLS" -gt "0" ]; then
            echo "âŒ Failed URLs: $FAILED_URLS (check logs for details)"
          fi
          echo "ğŸ“Š Attachments: $ATTACHMENTS_COUNT files"
          echo "ğŸ’¾ Data collected: $DATA_SIZE"
          echo "â±ï¸ Total time: ${DURATION_MIN}m ${DURATION_SEC}s"
          echo "ğŸ•’ Completed: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          
          mkdir -p data/logs
          cat > data/logs/final_stats.json << EOF
{
  "pages_processed": $SUCCESSFUL_PAGES,
  "urls_scraped": $SUCCESSFUL_URLS,
  "total_urls": $TOTAL_URLS,
  "success_rate": "$SUCCESS_RATE",
  "failed_urls": $FAILED_URLS,
  "attachments_count": $ATTACHMENTS_COUNT,
  "duration_minutes": $DURATION_MIN,
  "duration_seconds": $DURATION_SEC,
  "completed_at": "$(date -u '+%Y-%m-%d %H:%M:%S UTC')",
  "run_id": "${{ github.run_number }}"
}
EOF

      - name: ğŸ”„ Upload raw data to Supabase (dry run validation)
        run: |
          echo "ğŸ”„ Running Supabase upload dry run..."
          if [ -f "upload_raw_to_supabase.py" ]; then
            python upload_raw_to_supabase.py --dry-run --batch-size 25
            echo "âœ… Dry run validation completed"
          else
            echo "âš ï¸ upload_raw_to_supabase.py not found, skipping dry run"
          fi
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: ğŸš€ Upload raw data to Supabase (live upload)
        if: ${{ env.SCRAPER_DRY_RUN != 'true' }}
        run: |
          echo "ğŸš€ Starting live Supabase upload..."
          if [ -f "upload_raw_to_supabase.py" ]; then
            python upload_raw_to_supabase.py --batch-size 25 --verbose
            echo "âœ… Live upload completed"
          else
            echo "âš ï¸ upload_raw_to_supabase.py not found, skipping upload"
          fi
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: ğŸ“‹ Upload success validation with detailed metrics
        run: |
          echo "ğŸ“‹ === UPLOAD SUCCESS VALIDATION ==="
          
          if [ -f "data/logs/supabase_upload.log" ]; then
            echo "ğŸ“„ Upload log found - analyzing results:"
            
            UPLOADED_COUNT=$(grep -c "Successfully uploaded\|Upload successful" data/logs/supabase_upload.log 2>/dev/null || echo "0")
            ERROR_COUNT=$(grep -c "Upload failed\|ERROR" data/logs/supabase_upload.log 2>/dev/null || echo "0")
            SKIPPED_COUNT=$(grep -c "Skipped\|Already exists" data/logs/supabase_upload.log 2>/dev/null || echo "0")
            
            echo "  âœ… Successfully uploaded: $UPLOADED_COUNT records"
            echo "  âŒ Upload errors: $ERROR_COUNT records"
            echo "  â­ï¸ Skipped (duplicates): $SKIPPED_COUNT records"
            
            echo ""
            echo "ğŸ“Š Recent upload activity:"
            tail -5 data/logs/supabase_upload.log | while read line; do
              if echo "$line" | grep -q "Successfully\|SUCCESS"; then
                echo "  âœ… $line"
              elif echo "$line" | grep -q "ERROR\|Failed"; then
                echo "  âŒ $line"
              else
                echo "  â„¹ï¸ $line"
              fi
            done
            
            TOTAL_UPLOAD_ATTEMPTS=$((UPLOADED_COUNT + ERROR_COUNT))
            if [ "$TOTAL_UPLOAD_ATTEMPTS" -gt "0" ]; then
              UPLOAD_SUCCESS_RATE=$(awk "BEGIN {printf \"%.1f\", $UPLOADED_COUNT * 100 / $TOTAL_UPLOAD_ATTEMPTS}")
              echo ""
              echo "ğŸ“ˆ Upload success rate: ${UPLOAD_SUCCESS_RATE}% ($UPLOADED_COUNT/$TOTAL_UPLOAD_ATTEMPTS)"
            fi
          else
            echo "âš ï¸ Upload log not found at data/logs/supabase_upload.log"
          fi

      - name: ğŸ“¦ Upload raw_pages as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: raw_pages-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/raw_pages/
          retention-days: 30

      - name: ğŸ“¦ Upload attachments as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: attachments-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/attachments/
          retention-days: 30

      - name: ğŸ“¦ Upload logs and statistics as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: logs-and-stats-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/logs/
          retention-days: 14

      - name: ğŸ¯ Final Success Summary
        if: success()
        run: |
          echo "ğŸ¯ === WORKFLOW SUCCESS SUMMARY ==="
          echo "âœ… All steps completed successfully!"
          echo ""
          
          if [ -f "data/logs/final_stats.json" ]; then
            PAGES=$(jq -r '.pages_processed' data/logs/final_stats.json 2>/dev/null || echo "?")
            URLS=$(jq -r '.urls_scraped' data/logs/final_stats.json 2>/dev/null || echo "?")
            SUCCESS_RATE=$(jq -r '.success_rate' data/logs/final_stats.json 2>/dev/null || echo "?")
            DURATION_MIN=$(jq -r '.duration_minutes' data/logs/final_stats.json 2>/dev/null || echo "?")
            DURATION_SEC=$(jq -r '.duration_seconds' data/logs/final_stats.json 2>/dev/null || echo "?")
            ATTACHMENTS=$(jq -r '.attachments_count' data/logs/final_stats.json 2>/dev/null || echo "?")
            
            echo "ğŸ“Š SCRAPING RESULTS:"
            echo "  âœ… Pages processed: $PAGES"
            echo "  âœ… URLs scraped: $URLS"
            echo "  âœ… Success rate: $SUCCESS_RATE%"
            echo "  ğŸ“ Attachments: $ATTACHMENTS files"
            echo "  â±ï¸ Duration: ${DURATION_MIN}m ${DURATION_SEC}s"
          fi
          
          echo ""
          echo "ğŸ”§ SYSTEM STATUS:"
          echo "  âœ… Chrome/Chromedriver: Working"
          echo "  âœ… Virtual display: Working"  
          echo "  âœ… Test suite: Passed"
          echo "  âœ… Scraping: Completed"
          echo "  âœ… Supabase upload: Success"
          echo "  âœ… Artifacts: Uploaded"
          echo ""
          echo "ğŸ·ï¸ Run ID: ${{ github.run_number }}"
          echo "ğŸ“… Completed: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

      - name: âŒ Failure Debug & Recovery Info
        if: failure()
        run: |
          echo "âŒ === WORKFLOW FAILURE ANALYSIS ==="
          echo "âŒ Workflow failed - collecting comprehensive debug information"
          echo ""
          
          echo "ğŸ” SYSTEM ENVIRONMENT:"
          echo "  Python: $(python --version 2>&1)"
          echo "  DISPLAY: $DISPLAY"
          echo "  Working dir: $(pwd)"
          echo "  Disk space: $(df -h . | tail -1)"
          echo ""
          
          echo "ğŸ” PROCESS STATUS:"
          ps aux | grep -E "(Xvfb|chrome|python)" || echo "  No relevant processes found"
          echo ""
          
          echo "ğŸ” CHROME/DRIVER STATUS:"
          which chromium-browser chromedriver || echo "  Binaries not found"
          ls -la /usr/bin/chrome* 2>/dev/null || echo "  No chrome binaries in /usr/bin"
          echo ""
          
          echo "ğŸ” FILES STRUCTURE:"
          echo "  ğŸ“ Current directory contents:"
          ls -la . || echo "  Cannot list current directory"
          echo "  ğŸ“ Data directory status:"
          if [ -d "data" ]; then
            find data -type f -name "*.json" | head -5 | while read file; do
              SIZE=$(du -h "$file" 2>/dev/null | cut -f1 || echo "?")
              echo "    ğŸ“„ $file ($SIZE)"
            done
          else
            echo "    âŒ Data directory not found"
          fi
          echo ""
          
          echo "ğŸ” ERROR LOGS:"
          find data/logs -name "*.log" -exec echo "=== {} ===" \; -exec tail -10 {} \; 2>/dev/null || echo "  No log files found"
          echo ""
          
          echo "âŒ === DEBUG COLLECTION COMPLETE ==="
          echo "ğŸ’¡ Check the uploaded artifacts for detailed logs and partial results"
          echo "ğŸ’¡ Review the system environment section above for configuration issues"
          echo "ğŸ’¡ Run ID: ${{ github.run_number }} for reference"
