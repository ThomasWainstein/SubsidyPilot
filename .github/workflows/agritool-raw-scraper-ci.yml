# =============================================================================
# AgriTool Raw Scraper CI - IMPROVED WORKFLOW
# =============================================================================
# Improvements made:
# - Simplified pagination: max_pages instead of start_page/end_page
# - Auto-scrape when max_pages=0 (scrapes all available pages)
# - Enhanced progress tracking with individual URL reporting
# - Clear statistics: pages scraped, URLs processed, success rates
# - Professional summary reports with detailed metrics
# - Better error handling with URL-level failure tracking
# =============================================================================

name: ğŸ”¥ AgriTool Raw Scraper CI - SMART & CLEAR

on:
  push:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
      - '.github/workflows/AgriTool_Raw_Scraper_CI_Improved.yml'
  pull_request:
    paths:
      - 'AI_SCRAPER_RAW_TEXTS/**'
  workflow_dispatch:
    inputs:
      site:
        description: 'Target site to scrape'
        required: false
        default: 'franceagrimer'
        type: choice
        options:
          - franceagrimer
          - all
      max_pages:
        description: 'Number of pages to scrape (0 = scrape all available pages)'
        required: false
        default: '5'
        type: string
      max_urls_per_page:
        description: 'Maximum URLs to collect per page (0 = no limit)'
        required: false
        default: '0'
        type: string
      dry_run:
        description: 'Dry run upload (no actual Supabase insert)'
        required: false
        default: false
        type: boolean
  schedule:
    - cron: '0 6 * * *'

env:
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'
  
jobs:
  smart-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    defaults:
      run:
        working-directory: AI_SCRAPER_RAW_TEXTS
    steps:
      - name: ğŸ“‹ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: ğŸ”§ Install system dependencies (Chrome/Chromedriver/Xvfb)
        run: |
          echo "ğŸ”§ Installing system dependencies..."
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser chromium-chromedriver xvfb file wget curl jq bc
          echo "âœ… System dependencies installed"

      - name: ğŸ–¥ï¸ Setup virtual display (Xvfb)
        run: |
          echo "ğŸ–¥ï¸ Starting virtual display..."
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          export DISPLAY=:99
          sleep 3
          echo "âœ… Virtual display started on :99"
        env:
          DISPLAY: ':99'

      - name: ğŸ” Debug Chrome/Chromedriver installation
        run: |
          echo "ğŸ” === CHROME/CHROMEDRIVER DEBUG INFO ==="
          echo "ğŸ“ Chromium browser location:"
          which chromium-browser || echo "âŒ chromium-browser not found"
          if command -v chromium-browser >/dev/null 2>&1; then
            chromium-browser --version || echo "âŒ Cannot get Chromium version"
          fi
          
          echo "ğŸ“ Chromedriver location:"
          which chromedriver || echo "âŒ chromedriver not found"
          if command -v chromedriver >/dev/null 2>&1; then
            chromedriver --version || echo "âŒ Cannot get Chromedriver version"
          fi
          
          echo "ğŸ“ Display info:"
          echo "DISPLAY: $DISPLAY"
          ps aux | grep Xvfb || echo "âŒ Xvfb not running"
          echo "âœ… Debug info complete"

      - name: ğŸ§¹ Clear webdriver-manager cache
        run: |
          echo "ğŸ§¹ Clearing webdriver-manager cache..."
          rm -rf ~/.wdm || true
          rm -rf /home/runner/.wdm || true
          echo "âœ… Webdriver-manager cache cleared"

      - name: ğŸ“¦ Install Python dependencies
        run: |
          echo "ğŸ“¦ Installing Python dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "âœ… Python dependencies installed"
          echo "ğŸ“‹ Installed packages:"
          pip list | grep -E "(selenium|supabase|requests|beautifulsoup4)"

      - name: ğŸ” Validate environment variables
        run: |
          echo "ğŸ” Validating required Supabase environment variables..."
          python ../test_env_vars.py
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: ğŸ§ª Test system chromedriver directly
        run: |
          echo "ğŸ§ª Testing system chromedriver..."
          chromedriver --version
          python -c "
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          options = Options()
          options.add_argument('--headless=new')
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          driver = webdriver.Chrome(service=Service('/usr/bin/chromedriver'), options=options)
          print('âœ… System chromedriver test successful')
          driver.quit()
          "
          echo "âœ… System chromedriver validation completed"

      - name: ğŸ§ª Run comprehensive test suite
        run: |
          echo "ğŸ§ª Running test suite..."
          python test_scraper.py
          echo "âœ… Test suite completed successfully"

      - name: ğŸ“Š Display scraping configuration
        run: |
          echo "ğŸ“Š === SCRAPING CONFIGURATION ==="
          SITE="${{ github.event.inputs.site || 'franceagrimer' }}"
          MAX_PAGES="${{ github.event.inputs.max_pages || '5' }}"
          MAX_URLS="${{ github.event.inputs.max_urls_per_page || '0' }}"
          DRY_RUN="${{ github.event.inputs.dry_run || 'false' }}"
          
          echo "ğŸ¯ Target site: $SITE"
          if [ "$MAX_PAGES" = "0" ]; then
            echo "ğŸ“„ Pages to scrape: ALL available pages"
          else
            echo "ğŸ“„ Pages to scrape: $MAX_PAGES pages maximum"
          fi
          if [ "$MAX_URLS" = "0" ]; then
            echo "ğŸ”— URLs per page: no limit"
          else
            echo "ğŸ”— URLs per page: $MAX_URLS URLs maximum"
          fi
          echo "ğŸ§ª Dry run mode: $DRY_RUN"
          echo "â° Started at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

      - name: ğŸ•·ï¸ Execute smart scraping with progress tracking
        run: |
          echo "ğŸ•·ï¸ === STARTING SMART SCRAPING ==="
          SITE="${{ github.event.inputs.site || 'franceagrimer' }}"
          MAX_PAGES="${{ github.event.inputs.max_pages || '5' }}"
          MAX_URLS="${{ github.event.inputs.max_urls_per_page || '0' }}"
          
          if [ "$MAX_PAGES" = "0" ]; then
            echo "ğŸ”„ AUTO-SCRAPE MODE: Will scrape ALL available pages"
            CMD="python scraper_main.py --site $SITE --scrape-all --verbose --progress"
          else
            echo "ğŸ“Š LIMITED SCRAPE MODE: Will scrape maximum $MAX_PAGES pages"
            CMD="python scraper_main.py --site $SITE --max-pages $MAX_PAGES --verbose --progress"
          fi
          
          if [ "$MAX_URLS" != "0" ] && [ -n "$MAX_URLS" ]; then
            CMD="$CMD --max-urls-per-page $MAX_URLS"
            echo "ğŸ”— URL limit per page: $MAX_URLS"
          fi
          
          echo "ğŸš€ Executing: $CMD"
          echo ""
          
          START_TIME=$(date +%s)
          eval $CMD
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo ""
          echo "âœ… Scraping completed in ${DURATION} seconds"
          
          mkdir -p data/logs
          echo "{\"scraping_duration_seconds\": $DURATION, \"completed_at\": \"$(date -u '+%Y-%m-%d %H:%M:%S UTC')\"}" > data/logs/basic_stats.json

      - name: ğŸ“ˆ Detailed scraping results analysis
        run: |
          echo "ğŸ“ˆ === DETAILED SCRAPING RESULTS ==="
          
          JSON_FILES=$(find data/raw_pages -name "*.json" 2>/dev/null | wc -l || echo "0")
          ATTACHMENTS=$(find data/attachments -type f 2>/dev/null | wc -l || echo "0")
          
          echo "ğŸ“ Content files created:"
          echo "  ğŸ“„ JSON pages: $JSON_FILES files"
          echo "  ğŸ“ Attachments: $ATTACHMENTS files"
          
          if [ -d "data/raw_pages" ]; then
            TOTAL_SIZE=$(du -sh data/raw_pages 2>/dev/null | cut -f1 || echo "0B")
            echo "  ğŸ’¾ Total size: $TOTAL_SIZE"
          fi
          
          echo ""
          echo "ğŸ“‹ Sample of scraped content:"
          if [ "$JSON_FILES" -gt "0" ]; then
            find data/raw_pages -name "*.json" | head -10 | while read file; do
              if [ -f "$file" ]; then
                URL=$(jq -r '.url // .source_url // "Unknown URL"' "$file" 2>/dev/null || echo "Unknown URL")
                SIZE=$(du -h "$file" 2>/dev/null | cut -f1 || echo "?")
                TITLE=$(jq -r '.title // .page_title // "No title"' "$file" 2>/dev/null | head -c 60 || echo "No title")
                echo "  âœ… $URL ($SIZE) - $TITLE"
              fi
            done
          else
            echo "  âš ï¸ No JSON files found"
          fi
          
          echo ""
          echo "ğŸ” Error analysis:"
          if [ -f "data/logs/scraper.log" ]; then
            ERROR_COUNT=$(grep -c "ERROR\|FAILED" data/logs/scraper.log 2>/dev/null || echo "0")
            WARNING_COUNT=$(grep -c "WARNING\|WARN" data/logs/scraper.log 2>/dev/null || echo "0")
            echo "  âŒ Errors found: $ERROR_COUNT"
            echo "  âš ï¸ Warnings found: $WARNING_COUNT"
            
            if [ "$ERROR_COUNT" -gt "0" ]; then
              echo "  ğŸ“„ Recent errors:"
              grep "ERROR\|FAILED" data/logs/scraper.log | tail -5 | while read line; do
                echo "    âŒ $line"
              done
            fi
          else
            echo "  â„¹ï¸ No detailed error log found"
          fi

      - name: ğŸ“Š Generate comprehensive statistics
        run: |
          echo "ğŸ“Š === COMPREHENSIVE STATISTICS ==="
          
          TOTAL_PAGES=0
          SUCCESSFUL_PAGES=0
          TOTAL_URLS=0
          SUCCESSFUL_URLS=0
          FAILED_URLS=0
          
          if [ -d "data/raw_pages" ]; then
            TOTAL_PAGES=$(find data/raw_pages -name "page_*.json" 2>/dev/null | wc -l || echo "0")
            SUCCESSFUL_PAGES=$TOTAL_PAGES
            TOTAL_URLS=$(find data/raw_pages -name "*.json" ! -name "page_*.json" 2>/dev/null | wc -l || echo "0")
            SUCCESSFUL_URLS=$TOTAL_URLS
          fi
          
          if [ "$TOTAL_URLS" -gt "0" ]; then
            SUCCESS_RATE=$(echo "scale=1; $SUCCESSFUL_URLS * 100 / $TOTAL_URLS" | bc -l 2>/dev/null || echo "100.0")
          else
            SUCCESS_RATE="0.0"
          fi
          
          if [ -f "data/logs/basic_stats.json" ]; then
            DURATION=$(jq -r '.scraping_duration_seconds // 0' data/logs/basic_stats.json 2>/dev/null || echo "0")
            DURATION_MIN=$((DURATION / 60))
            DURATION_SEC=$((DURATION % 60))
          else
            DURATION_MIN=0
            DURATION_SEC=0
          fi
          
          echo "ğŸ¯ === FINAL SCRAPING REPORT ==="
          echo "âœ… Pages processed: $SUCCESSFUL_PAGES pages"
          echo "âœ… URLs scraped: $SUCCESSFUL_URLS total URLs"
          echo "âœ… Success rate: ${SUCCESS_RATE}% ($SUCCESSFUL_URLS/$TOTAL_URLS URLs)"
          if [ "$FAILED_URLS" -gt "0" ]; then
            echo "âŒ Failed URLs: $FAILED_URLS (check logs for details)"
          fi
          echo "ğŸ“Š Attachments: $(find data/attachments -type f 2>/dev/null | wc -l || echo "0") files"
          echo "ğŸ’¾ Data collected: $(du -sh data/ 2>/dev/null | cut -f1 || echo "0B")"
          echo "â±ï¸ Total time: ${DURATION_MIN}m ${DURATION_SEC}s"
          echo "ğŸ•’ Completed: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          
          mkdir -p data/logs
          cat > data/logs/final_stats.json << EOF
{
  "pages_processed": $SUCCESSFUL_PAGES,
  "urls_scraped": $SUCCESSFUL_URLS,
  "total_urls": $TOTAL_URLS,
  "success_rate": $SUCCESS_RATE,
  "failed_urls": $FAILED_URLS,
  "attachments_count": $(find data/attachments -type f 2>/dev/null | wc -l || echo "0"),
  "duration_minutes": $DURATION_MIN,
  "duration_seconds": $DURATION_SEC,
  "completed_at": "$(date -u '+%Y-%m-%d %H:%M:%S UTC')",
  "run_id": "${{ github.run_number }}"
}
EOF

      - name: ğŸ”„ Upload raw data to Supabase (dry run validation)
        run: |
          echo "ğŸ”„ Running Supabase upload dry run..."
          python upload_raw_to_supabase.py --dry-run --batch-size 25
          echo "âœ… Dry run validation completed"
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: ğŸš€ Upload raw data to Supabase (live upload)
        if: ${{ github.event.inputs.dry_run != 'true' }}
        run: |
          echo "ğŸš€ Starting live Supabase upload..."
          python upload_raw_to_supabase.py --batch-size 25 --verbose
          echo "âœ… Live upload completed"
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        continue-on-error: false

      - name: ğŸ“‹ Upload success validation with detailed metrics
        run: |
          echo "ğŸ“‹ === UPLOAD SUCCESS VALIDATION ==="
          
          if [ -f "data/logs/supabase_upload.log" ]; then
            echo "ğŸ“„ Upload log found - analyzing results:"
            
            UPLOADED_COUNT=$(grep -c "Successfully uploaded\|Upload successful" data/logs/supabase_upload.log 2>/dev/null || echo "0")
            ERROR_COUNT=$(grep -c "Upload failed\|ERROR" data/logs/supabase_upload.log 2>/dev/null || echo "0")
            SKIPPED_COUNT=$(grep -c "Skipped\|Already exists" data/logs/supabase_upload.log 2>/dev/null || echo "0")
            
            echo "  âœ… Successfully uploaded: $UPLOADED_COUNT records"
            echo "  âŒ Upload errors: $ERROR_COUNT records"
            echo "  â­ï¸ Skipped (duplicates): $SKIPPED_COUNT records"
            
            echo ""
            echo "ğŸ“Š Recent upload activity:"
            tail -10 data/logs/supabase_upload.log | while read line; do
              if echo "$line" | grep -q "Successfully\|SUCCESS"; then
                echo "  âœ… $line"
              elif echo "$line" | grep -q "ERROR\|Failed"; then
                echo "  âŒ $line"
              else
                echo "  â„¹ï¸ $line"
              fi
            done
            
            TOTAL_UPLOAD_ATTEMPTS=$((UPLOADED_COUNT + ERROR_COUNT))
            if [ "$TOTAL_UPLOAD_ATTEMPTS" -gt "0" ]; then
              UPLOAD_SUCCESS_RATE=$(echo "scale=1; $UPLOADED_COUNT * 100 / $TOTAL_UPLOAD_ATTEMPTS" | bc -l 2>/dev/null || echo "100.0")
              echo ""
              echo "ğŸ“ˆ Upload success rate: ${UPLOAD_SUCCESS_RATE}% ($UPLOADED_COUNT/$TOTAL_UPLOAD_ATTEMPTS)"
            fi
          else
            echo "âš ï¸ Upload log not found at data/logs/supabase_upload.log"
          fi

      - name: ğŸ“¦ Upload raw_pages as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: raw_pages-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/raw_pages/
          retention-days: 30

      - name: ğŸ“¦ Upload attachments as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: attachments-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/attachments/
          retention-days: 30

      - name: ğŸ“¦ Upload logs and statistics as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: logs-and-stats-${{ github.run_number }}
          path: AI_SCRAPER_RAW_TEXTS/data/logs/
          retention-days: 14

      - name: ğŸ¯ Final Success Summary
        if: success()
        run: |
          echo "ğŸ¯ === WORKFLOW SUCCESS SUMMARY ==="
          echo "âœ… All steps completed successfully!"
          echo ""
          
          if [ -f "data/logs/final_stats.json" ]; then
            PAGES=$(jq -r '.pages_processed' data/logs/final_stats.json 2>/dev/null || echo "?")
            URLS=$(jq -r '.urls_scraped' data/logs/final_stats.json 2>/dev/null || echo "?")
            SUCCESS_RATE=$(jq -r '.success_rate' data/logs/final_stats.json 2>/dev/null || echo "?")
            DURATION_MIN=$(jq -r '.duration_minutes' data/logs/final_stats.json 2>/dev/null || echo "?")
            DURATION_SEC=$(jq -r '.duration_seconds' data/logs/final_stats.json 2>/dev/null || echo "?")
            ATTACHMENTS=$(jq -r '.attachments_count' data/logs/final_stats.json 2>/dev/null || echo "?")
            
            echo "ğŸ“Š SCRAPING RESULTS:"
            echo "  âœ… Pages processed: $PAGES"
            echo "  âœ… URLs scraped: $URLS"
            echo "  âœ… Success rate: $SUCCESS_RATE%"
            echo "  ğŸ“ Attachments: $ATTACHMENTS files"
            echo "  â±ï¸ Duration: ${DURATION_MIN}m ${DURATION_SEC}s"
          fi
          
          echo ""
          echo "ğŸ”§ SYSTEM STATUS:"
          echo "  âœ… Chrome/Chromedriver: Working"
          echo "  âœ… Virtual display: Working"  
          echo "  âœ… Test suite: Passed"
          echo "  âœ… Scraping: Completed"
          echo "  âœ… Supabase upload: Success"
          echo "  âœ… Artifacts: Uploaded"
          echo ""
          echo "ğŸ·ï¸ Run ID: ${{ github.run_number }}"
          echo "ğŸ“… Completed: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

      - name: âŒ Failure Debug & Recovery Info
        if: failure()
        run: |
          echo "âŒ === WORKFLOW FAILURE ANALYSIS ==="
          echo "âŒ Workflow failed - collecting comprehensive debug information"
          echo ""
          
          echo "ğŸ” SYSTEM ENVIRONMENT:"
          echo "  Python: $(python --version 2>&1)"
          echo "  DISPLAY: $DISPLAY"
          echo "  Working dir: $(pwd)"
          echo "  Disk space: $(df -h . | tail -1)"
          echo ""
          
          echo "ğŸ” PROCESS STATUS:"
          ps aux | grep -E "(Xvfb|chrome|python)" || echo "  No relevant processes found"
          echo ""
          
          echo "ğŸ” CHROME/DRIVER STATUS:"
          which chromium-browser chromedriver || echo "  Binaries not found"
          ls -la /usr/bin/chrome* 2>/dev/null || echo "  No chrome binaries in /usr/bin"
          echo ""
          
          echo "ğŸ” DATA DIRECTORY STATUS:"
          if [ -d "data" ]; then
            echo "  ğŸ“ Data directory structure:"
            find data -type f -name "*.json" | head -10 | while read file; do
              SIZE=$(du -h "$file" 2>/dev/null | cut -f1 || echo "?")
              echo "    ğŸ“„ $file ($SIZE)"
            done
            echo "    ... (showing first 10 JSON files)"
          else
            echo "  âŒ Data directory not found"
          fi
          echo ""
          
          echo "ğŸ” ERROR LOGS:"
          find data/logs -name "*.log" -exec echo "=== {} ===" \; -exec tail -20 {} \; 2>/dev/null || echo "  No log files found"
          echo ""
          
          echo "ğŸ” RECENT PYTHON ERRORS:"
          journalctl --since "10 minutes ago" | grep -i python | tail -10 || echo "  No recent Python errors in system journal"
          echo ""
          
          echo "âŒ === DEBUG COLLECTION COMPLETE ==="
          echo "ğŸ’¡ Check the uploaded artifacts for detailed logs and partial results"
          echo "ğŸ’¡ Review the system environment section above for configuration issues"
          echo "ğŸ’¡ Run ID: ${{ github.run_number }} for reference"
