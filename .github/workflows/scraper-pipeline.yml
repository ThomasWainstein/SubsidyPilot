name: üï∑Ô∏è Scraper Pipeline

on:
  workflow_dispatch:
    inputs:
      site:
        description: 'Target site (default: franceagrimer)'
        required: false
        default: franceagrimer
      max-pages:
        description: 'Maximum pages to scrape'
        required: false
        default: '5'
      dry-run:
        description: 'Enable dry-run mode (results not saved)'
        required: false
        type: boolean
        default: false
  schedule:
    - cron: '0 6 * * *' # Daily at 06:00 UTC

env:
  PYTHONUNBUFFERED: "1"
  SELENIUM_HEADLESS: "1"

jobs:
  validate:
    name: Validate YAML & Python Environment
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: YAML Lint
        uses: ibiqlik/action-yamllint@v3
        with:
          file_or_dir: "."
          config_data: |
            extends: default

      - name: Python Syntax Check (src only)
        run: |
          python -m compileall -q main.py core.py multi_tab_extractor.py ai_extractor.py || exit 1

      - name: Validate Selenium Compliance
        run: |
          python validate_selenium_compliance.py

  scrape:
    name: Scrape ${{ github.event.inputs.site || 'franceagrimer' }}
    runs-on: ubuntu-latest
    needs: validate
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install selenium

      - name: Make output directory
        run: mkdir -p output

      - name: Validate Pipeline Pre-run
        run: |
          python validate_pipeline.py --mode scraping \
            --site "${{ github.event.inputs.site || 'franceagrimer' }}" \
            --max-pages "${{ github.event.inputs.max-pages || 5 }}"

      - name: Run Scraper
        env:
          SCRAPER_SITE: ${{ github.event.inputs.site || 'franceagrimer' }}
          SCRAPER_MAX_PAGES: ${{ github.event.inputs.max-pages || 5 }}
          SCRAPER_DRY_RUN: ${{ github.event.inputs.dry-run }}
        run: |
          python main.py --mode scraping \
            --site "$SCRAPER_SITE" \
            --max-pages "$SCRAPER_MAX_PAGES" \
            $([ "$SCRAPER_DRY_RUN" = "true" ] && echo "--dry-run" || true)

      - name: Upload Results Artifact
        if: success() && !cancelled() && github.event.inputs.dry-run != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output
          path: output/
