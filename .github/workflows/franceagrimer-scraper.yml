name: FranceAgriMer Subsidy Scraper

on:
  # Weekly schedule: Every Sunday at 01:00 UTC
  schedule:
    - cron: '0 1 * * 0'
  
  # Manual trigger for immediate execution
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape (0 = unlimited)'
        required: false
        default: '0'
        type: string
      dry_run:
        description: 'Dry run mode (no database writes)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

jobs:
  scrape-franceagrimer:
    runs-on: ubuntu-latest
    
    steps:
      - name: Print runner/system context
        run: |
          echo "GITHUB_WORKSPACE: $GITHUB_WORKSPACE"
          echo "RUNNER_OS: $RUNNER_OS"
          echo "HOSTNAME: $(hostname)"
          lsb_release -a || true
          env | sort
          df -h
          free -h
          which python3
          python3 --version
          pip --version

      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            chromium-browser \
            chromium-chromedriver \
            python3-pip \
            python3-venv
            
      - name: Install Python dependencies
        working-directory: ./AgriToolScraper-main
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Configure Chrome for headless operation
        run: |
          export CHROME_BIN=/usr/bin/chromium-browser
          export CHROMEDRIVER_BIN=/usr/bin/chromedriver
          
      - name: Set up environment variables
        run: |
          echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" >> $GITHUB_ENV
          echo "SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" >> $GITHUB_ENV
          echo "CHROME_BIN=/usr/bin/chromium-browser" >> $GITHUB_ENV
          echo "CHROMEDRIVER_BIN=/usr/bin/chromedriver" >> $GITHUB_ENV
          
      - name: Clear Python cache
        working-directory: ./AgriToolScraper-main
        run: |
          set -xe
          python clear_cache.py >clear_cache.log 2>&1 || (cat clear_cache.log; exit 1)
          
      - name: Run Selenium compliance test
        working-directory: ./AgriToolScraper-main
        run: |
          set -xe
          python test_driver_compliance.py >compliance_test.log 2>&1 || (cat compliance_test.log; exit 1)
          
      - name: Execute FranceAgriMer scraper
        working-directory: ./AgriToolScraper-main
        run: |
          set -xe
          python main.py \
            --url "https://www.franceagrimer.fr/Accompagner/Dispositifs-par-filiere/Aides-nationales" \
            --max_pages ${{ github.event.inputs.max_pages || '0' }} \
            --dry_run ${{ github.event.inputs.dry_run || 'false' }} \
            >scraper_main.log 2>&1 || (cat scraper_main.log; exit 1)
            
      - name: Upload scraper logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            AgriToolScraper-main/logs/
            AgriToolScraper-main/data/extracted/
            AgriToolScraper-main/data/raw_pages/
          retention-days: 30
          
      - name: Upload error screenshots
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: error-screenshots-${{ github.run_number }}
          path: |
            AgriToolScraper-main/data/extracted/error_*.png
          retention-days: 7
          
      - name: Print log tail on failure
        if: failure()
        run: |
          echo "===== LOG TAILS (last 50 lines each) ====="
          tail -n 50 AgriToolScraper-main/logs/* || true
          tail -n 50 AgriToolScraper-main/data/extracted/*log* || true
          tail -n 50 AgriToolScraper-main/*.log || true
          echo "===== Directory structure ====="
          find AgriToolScraper-main/

      - name: Dump all outputs and logs on failure
        if: failure()
        run: |
          echo "======= DIRECTORY STRUCTURE ======="
          find AgriToolScraper-main/
          echo "======= LOG DUMP ======="
          for f in AgriToolScraper-main/logs/*; do echo "=== $f ==="; tail -n 100 "$f" || true; done
          for f in AgriToolScraper-main/data/extracted/*; do echo "=== $f ==="; tail -n 100 "$f" || true; done
          for f in AgriToolScraper-main/*.log; do echo "=== $f ==="; tail -n 100 "$f" || true; done

      - name: Notify on completion
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "‚úÖ FranceAgriMer scraper completed successfully"
            echo "üìä Check Supabase database for new subsidy records"
          else
            echo "‚ùå FranceAgriMer scraper failed"
            echo "üîç Check artifacts for logs and error details"
          fi