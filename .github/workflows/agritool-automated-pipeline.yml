# =============================================================================
# AgriTool Enhanced Automated End-to-End Pipeline
# =============================================================================
# Comprehensive automation workflow with modern architecture that handles:
# 1. URL Discovery from FranceAgriMer using enhanced discovery module
# 2. Parallel data scraping with robust error handling
# 3. Async raw data upload to Supabase raw_scraped_pages table  
# 4. Enhanced AI extraction with quality assessment
# 5. Structured data population with validation
# 6. Comprehensive quality assurance and monitoring
# 7. Performance optimization and resource management
# =============================================================================

name: AgriTool Enhanced Automated Pipeline

on:
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape (0 = unlimited)'
        required: false
        default: '50'
        type: string
      dry_run:
        description: 'Dry run mode (no database writes)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      run_tests:
        description: 'Run comprehensive test suite'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'
      scraper_mode:
        description: 'Scraper operation mode'
        required: false
        default: 'scrape'
        type: choice
        options:
          - 'discover'
          - 'scrape'
          - 'test'
      max_workers:
        description: 'Maximum concurrent workers for scraping'
        required: false
        default: '5'
        type: string
      ai_batch_size:
        description: 'AI extraction batch size'
        required: false
        default: '10'
        type: string
      ai_model:
        description: 'AI model to use for extraction'
        required: false
        default: 'gpt-4-turbo-preview'
        type: choice
        options:
          - 'gpt-4-turbo-preview'
          - 'gpt-4'
          - 'gpt-4o'
      quality_threshold:
        description: 'Minimum quality threshold for extractions'
        required: false
        default: '70'
        type: string
      enable_monitoring:
        description: 'Enable real-time monitoring'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

  schedule:
    # Run daily at 2 AM UTC for full automation
    - cron: '0 2 * * *'
    # Run weekly comprehensive scan on Sundays at 1 AM UTC
    - cron: '0 1 * * 0'

  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.11'
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'
  PYTHONPATH: '.'

jobs:
  # =============================================================================
  # JOB 1: ENVIRONMENT SETUP & VALIDATION
  # =============================================================================
  setup-and-validate:
    name: üîß Setup & Environment Validation
    runs-on: ubuntu-24.04
    
    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      OPENAI_API_KEY: ${{ secrets.SCRAPPER_RAW_GPT_API }}

    outputs:
      setup-success: ${{ steps.setup.outputs.success }}

    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: üêç Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: üìÅ Create pip cache directory
        run: |
          mkdir -p ~/.cache/pip
          echo "‚úÖ Pip cache directory created"

      - name: üíæ Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: Linux-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            Linux-pip-

      - name: üåê Install Chrome and dependencies
        run: |
          echo "üì¶ Installing Chrome and system dependencies..."
          sudo apt-get update
          sudo apt-get install -y \
            wget \
            gnupg \
            software-properties-common \
            apt-transport-https \
            ca-certificates \
            curl \
            xvfb \
            unzip
          
          # Install Chrome using the official method for GitHub Actions
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Verify Chrome installation and show path
          which google-chrome || which google-chrome-stable
          google-chrome --version || google-chrome-stable --version
          
          # Create symlink if needed
          if [ ! -f /usr/bin/google-chrome ]; then
            sudo ln -s /usr/bin/google-chrome-stable /usr/bin/google-chrome
          fi
          
          # Install ChromeDriver using the new method
          # Get Chrome version
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+' | head -1)
          CHROME_MAJOR_VERSION=$(echo $CHROME_VERSION | cut -d. -f1)
          
          # Download appropriate ChromeDriver
          if [ "$CHROME_MAJOR_VERSION" -ge "115" ]; then
            # For Chrome 115+ use Chrome for Testing
            CHROMEDRIVER_URL="https://googlechromelabs.github.io/chrome-for-testing/latest-versions-per-milestone-with-downloads.json"
            wget -O chromedriver.json "$CHROMEDRIVER_URL"
            DRIVER_URL=$(python3 -c "
import json, sys
with open('chromedriver.json', 'r') as f:
    data = json.load(f)
    try:
        milestone = '$CHROME_MAJOR_VERSION'
        url = data['milestones'][milestone]['downloads']['chromedriver'][0]['url']
        if 'linux64' in url:
            print(url)
        else:
            for item in data['milestones'][milestone]['downloads']['chromedriver']:
                if 'linux64' in item['url']:
                    print(item['url'])
                    break
    except:
        sys.exit(1)
")
            if [ -n "$DRIVER_URL" ]; then
              wget -O chromedriver-linux64.zip "$DRIVER_URL"
              unzip chromedriver-linux64.zip
              sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
              sudo chmod +x /usr/local/bin/chromedriver
            fi
          else
            # For older Chrome versions
            wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_MAJOR_VERSION}/chromedriver_linux64.zip"
            sudo unzip /tmp/chromedriver.zip -d /usr/local/bin/
            sudo chmod +x /usr/local/bin/chromedriver
          fi
          
          # Verify installations
          echo "Chrome version: $(google-chrome --version)"
          echo "ChromeDriver version: $(chromedriver --version)"
          echo "Chrome binary location: $(which google-chrome)"
          echo "‚úÖ Chrome and ChromeDriver installed successfully"

      - name: üîß Install System Dependencies
        run: |
          echo "üì¶ Installing additional system dependencies..."
          
          # Document processing dependencies
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-fra \
            poppler-utils \
            ghostscript \
            libxml2-dev \
            libxslt1-dev \
            libffi-dev \
            libjpeg-dev \
            libpng-dev \
            libmagic1 \
            libreoffice \
            pandoc \
            file
          
          # Performance monitoring tools
          sudo apt-get install -y htop iotop
          
          echo "‚úÖ Enhanced system dependencies installed"

      - name: üñ•Ô∏è Start Virtual Display
        run: |
          # Start virtual display with optimized settings
          Xvfb :99 -screen 0 1920x1080x24 -ac -nolisten tcp -dpi 96 &
          sleep 3
          echo "‚úÖ Virtual display configured"

      - name: üì¶ Install Python Dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          
          # Install requirements if they exist
          if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
            echo "‚úÖ Requirements.txt installed"
          fi
          
          # Install additional common scraping dependencies
          pip install \
            selenium \
            beautifulsoup4 \
            requests \
            lxml \
            undetected-chromedriver \
            webdriver-manager \
            pandas \
            python-dotenv \
            supabase \
            openai \
            aiofiles \
            aiohttp \
            psutil
          
          echo "‚úÖ Python dependencies installed"

      - name: ‚úÖ Environment Validation
        id: setup
        run: |
          echo "üîç Validating environment setup..."
          
          # Check required environment variables
          MISSING_VARS=""
          
          [ -z "$SUPABASE_URL" ] && MISSING_VARS="$MISSING_VARS SUPABASE_URL"
          [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] && MISSING_VARS="$MISSING_VARS SUPABASE_SERVICE_ROLE_KEY"
          [ -z "$SCRAPER_RAW_GPT_API" ] && MISSING_VARS="$MISSING_VARS SCRAPER_RAW_GPT_API"
          
          if [ -n "$MISSING_VARS" ]; then
            echo "‚ùå Missing required environment variables:$MISSING_VARS"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check browser availability
          google-chrome --version || chromium --version || {
            echo "‚ùå No suitable browser found"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          }
          
          # Check display
          if [ -z "$DISPLAY" ]; then
            echo "‚ùå DISPLAY not set"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Test Python imports
          python -c "
          import sys
          try:
              import selenium
              import requests
              import bs4
              import supabase
              import openai
              print('‚úÖ All Python imports successful')
              print(f'Python version: {sys.version}')
              print(f'Selenium version: {selenium.__version__}')
          except ImportError as e:
              print(f'‚ùå Import error: {e}')
              exit(1)
          "
          
          echo "success=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Environment validation successful"

      - name: üß™ Test browser setup
        run: |
          echo "üß™ Testing browser functionality..."
          python -c "
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import sys
import os
try:
    print('üîç Checking Chrome installation...')
    # Check if Chrome binary exists
    chrome_paths = ['/usr/bin/google-chrome', '/usr/bin/google-chrome-stable']
    chrome_binary = None
    for path in chrome_paths:
        if os.path.exists(path):
            chrome_binary = path
            print(f'‚úÖ Found Chrome at: {chrome_binary}')
            break
    
    if not chrome_binary:
        raise Exception('Chrome binary not found in expected locations')
    
    # Configure Chrome options
    print('‚öôÔ∏è Configuring Chrome options...')
    options = Options()
    options.add_argument('--headless=new')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-web-security')
    options.add_argument('--allow-running-insecure-content')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--disable-javascript')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
    
    # Set binary location if not default
    if chrome_binary != '/usr/bin/google-chrome':
        options.binary_location = chrome_binary
    
    # Setup ChromeDriver service
    print('üöÄ Starting Chrome driver...')
    service = Service('/usr/local/bin/chromedriver')
    
    # Test Chrome
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_page_load_timeout(30)
    
    print('üåê Testing web request...')
    driver.get('https://httpbin.org/user-agent')
    print(f'‚úÖ Chrome test successful - Page title: {driver.title}')
    print(f'üìÑ Page source length: {len(driver.page_source)} characters')
    
    driver.quit()
    print('üéâ Browser setup test completed successfully!')
    
except Exception as e:
    print(f'‚ùå Browser test failed: {str(e)}')
    print('üîß Debugging information:')
    import subprocess
    import shutil
    
    # Check what's actually installed
    chrome_check = subprocess.run(['which', 'google-chrome'], capture_output=True, text=True)
    chrome_stable_check = subprocess.run(['which', 'google-chrome-stable'], capture_output=True, text=True)
    chromedriver_check = subprocess.run(['which', 'chromedriver'], capture_output=True, text=True)
    
    print(f'google-chrome location: {chrome_check.stdout.strip() if chrome_check.returncode == 0 else \"Not found\"}')
    print(f'google-chrome-stable location: {chrome_stable_check.stdout.strip() if chrome_stable_check.returncode == 0 else \"Not found\"}')
    print(f'chromedriver location: {chromedriver_check.stdout.strip() if chromedriver_check.returncode == 0 else \"Not found\"}')
    
    # Try to get version info
    try:
        version_check = subprocess.run(['google-chrome', '--version'], capture_output=True, text=True)
        print(f'Chrome version: {version_check.stdout.strip()}')
    except:
        try:
            version_check = subprocess.run(['google-chrome-stable', '--version'], capture_output=True, text=True)
            print(f'Chrome stable version: {version_check.stdout.strip()}')
        except:
            print('Could not get Chrome version')
    
    sys.exit(1)
"
          echo "‚úÖ Browser test completed successfully"

  # =============================================================================
  # JOB 2: ENHANCED SCRAPING & DATA DISCOVERY
  # =============================================================================
  enhanced-scraping:
    name: üï∑Ô∏è Enhanced Scraping & Discovery
    runs-on: ubuntu-latest
    needs: setup-and-validate
    if: needs.setup-and-validate.outputs.setup-success == 'true'
    timeout-minutes: 120
    outputs:
      scrape-success: ${{ steps.scraping.outputs.success }}
      records-processed: ${{ steps.scraping.outputs.records }}
      urls-discovered: ${{ steps.scraping.outputs.urls_discovered }}
      
    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      OPENAI_API_KEY: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      DISPLAY: ':99'
      
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üñ•Ô∏è Setup virtual display
        run: |
          Xvfb :99 -screen 0 1920x1080x24 -ac -nolisten tcp &
          sleep 3
        env:
          DISPLAY: ':99'

      - name: üìÅ Create enhanced directory structure
        working-directory: ./AgriToolScraper-main
        run: |
          mkdir -p {data/scraped,data/extracted,logs,debug,artifacts}
          mkdir -p {data/urls,data/raw,data/processed}
          echo "‚úÖ Enhanced directory structure created"

      - name: üì¶ Install scraper dependencies with caching
        working-directory: ./AgriToolScraper-main  
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
          # Install additional async dependencies
          pip install aiofiles aiohttp psutil
          
          echo "‚úÖ All dependencies installed"

      - name: üï∑Ô∏è Enhanced scraping with modern runner
        id: scraping
        working-directory: ./AgriToolScraper-main
        run: |
          echo "üöÄ Starting enhanced scraping pipeline..."
          
          # Determine scraping mode
          SCRAPE_MODE="${{ github.event.inputs.scraper_mode || 'scrape' }}"
          MAX_WORKERS="${{ github.event.inputs.max_workers || '5' }}"
          MAX_PAGES="${{ github.event.inputs.max_pages || '50' }}"
          
          echo "üìã Configuration:"
          echo "  Mode: $SCRAPE_MODE"
          echo "  Max workers: $MAX_WORKERS"
          echo "  Max pages: $MAX_PAGES"
          echo "  Dry run: ${{ github.event.inputs.dry_run || 'false' }}"
          
          # Build enhanced runner command
          RUNNER_CMD="python runner.py --mode $SCRAPE_MODE --max-workers $MAX_WORKERS"
          
          # Add browser selection
          RUNNER_CMD="$RUNNER_CMD --browser chrome --headless"
          
          # Add pages limit if specified
          if [ "$MAX_PAGES" != "0" ]; then
            RUNNER_CMD="$RUNNER_CMD --max-pages $MAX_PAGES"
          fi
          
          # Add dry run if specified
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            RUNNER_CMD="$RUNNER_CMD --dry-run"
          fi
          
          # Add monitoring if enabled
          if [ "${{ github.event.inputs.enable_monitoring || 'true' }}" = "true" ]; then
            RUNNER_CMD="$RUNNER_CMD --verbose"
          fi
          
          echo "üöÄ Executing: $RUNNER_CMD"
          
          # Execute with timeout and monitoring
          timeout 7200 $RUNNER_CMD || {
            EXIT_CODE=$?
            if [ $EXIT_CODE -eq 124 ]; then
              echo "‚ö†Ô∏è Scraping timed out after 2 hours"
            else
              echo "‚ùå Scraping failed with exit code: $EXIT_CODE"
            fi
            echo "success=false" >> $GITHUB_OUTPUT
            exit $EXIT_CODE
          }
          
          # Parse results from logs and output files
          RECORDS_PROCESSED=0
          URLS_DISCOVERED=0
          
          # Count processed records from various sources
          if [ -d "data/scraped" ]; then
            SCRAPED_FILES=$(find data/scraped -name "*.json" | wc -l)
            RECORDS_PROCESSED=$((RECORDS_PROCESSED + SCRAPED_FILES))
          fi
          
          if [ -d "data/urls" ]; then
            URL_FILES=$(find data/urls -name "*.txt" -o -name "*.json" | wc -l)
            URLS_DISCOVERED=$((URLS_DISCOVERED + URL_FILES))
          fi
          
          # Check for discovery results
          if [ -f "discovered_urls.json" ]; then
            DISCOVERED_COUNT=$(python -c "
            import json
            try:
                with open('discovered_urls.json', 'r') as f:
                    data = json.load(f)
                    print(len(data.get('urls', [])))
            except:
                print(0)
            " 2>/dev/null || echo "0")
            URLS_DISCOVERED=$((URLS_DISCOVERED + DISCOVERED_COUNT))
          fi
          
          echo "üìä Scraping Results:"
          echo "  Records processed: $RECORDS_PROCESSED"
          echo "  URLs discovered: $URLS_DISCOVERED"
          
          # Set outputs
          echo "records=$RECORDS_PROCESSED" >> $GITHUB_OUTPUT
          echo "urls_discovered=$URLS_DISCOVERED" >> $GITHUB_OUTPUT
          
          # Determine success based on results or dry run
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Dry run completed successfully"
          elif [ "$RECORDS_PROCESSED" -gt "0" ] || [ "$URLS_DISCOVERED" -gt "0" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Scraping completed successfully"
          else
            echo "success=false" >> $GITHUB_OUTPUT
            echo "‚ùå No records processed - scraping may have failed"
          fi

      - name: üìä Upload scraping artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: enhanced-scraper-output-${{ github.run_number }}
          path: |
            AgriToolScraper-main/data/
            AgriToolScraper-main/logs/
            AgriToolScraper-main/artifacts/
            AgriToolScraper-main/*.log
            AgriToolScraper-main/*.json
          retention-days: 14

  # =============================================================================
  # JOB 3: ENHANCED AI EXTRACTION
  # =============================================================================
  enhanced-ai-extraction:
    name: ü§ñ Enhanced AI Extraction
    runs-on: ubuntu-latest
    needs: [setup-and-validate, enhanced-scraping]
    if: needs.enhanced-scraping.outputs.scrape-success == 'true'
    timeout-minutes: 180
    outputs:
      extraction-success: ${{ steps.ai-extraction.outputs.success }}
      extracted-count: ${{ steps.ai-extraction.outputs.extracted }}
      quality-score: ${{ steps.ai-extraction.outputs.quality_score }}

    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      OPENAI_API_KEY: ${{ secrets.SCRAPPER_RAW_GPT_API }}

    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install AI extraction dependencies
        working-directory: ./AgriToolScraper-main
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
          # Install additional async dependencies for enhanced extractor
          pip install aiofiles psutil asyncpg
          
          echo "‚úÖ AI extraction dependencies installed"

      - name: ü§ñ Run enhanced AI extraction
        id: ai-extraction
        working-directory: ./AgriToolScraper-main
        run: |
          echo "ü§ñ Starting enhanced AI extraction..."
          
          # Configuration from inputs
          BATCH_SIZE="${{ github.event.inputs.ai_batch_size || '10' }}"
          MODEL="${{ github.event.inputs.ai_model || 'gpt-4-turbo-preview' }}"
          QUALITY_THRESHOLD="${{ github.event.inputs.quality_threshold || '70' }}"
          MAX_RECORDS="${{ github.event.inputs.max_pages || '0' }}"
          
          echo "üìã AI Extraction Configuration:"
          echo "  Model: $MODEL"
          echo "  Batch size: $BATCH_SIZE"
          echo "  Quality threshold: $QUALITY_THRESHOLD"
          echo "  Max records: $MAX_RECORDS"
          
          # Build AI extraction command using enhanced extractor
          AI_CMD="python ai_extractor.py --model $MODEL --batch-size $BATCH_SIZE --quality-threshold $QUALITY_THRESHOLD"
          
          # Add max records if specified
          if [ "$MAX_RECORDS" != "0" ]; then
            AI_CMD="$AI_CMD --max-records $MAX_RECORDS"
          fi
          
          # Add monitoring if enabled
          if [ "${{ github.event.inputs.enable_monitoring || 'true' }}" = "true" ]; then
            AI_CMD="$AI_CMD --verbose"
          fi
          
          # Add debug mode if dry run
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            AI_CMD="$AI_CMD --debug --output both"
          fi
          
          echo "üöÄ Executing: $AI_CMD"
          
          # Execute with comprehensive error handling
          {
            eval $AI_CMD
            EXTRACTION_EXIT_CODE=$?
          } || {
            EXTRACTION_EXIT_CODE=$?
            echo "‚ùå AI extraction failed with exit code: $EXTRACTION_EXIT_CODE"
          }
          
          # Parse extraction results
          EXTRACTED_COUNT=0
          QUALITY_SCORE=0
          
          # Check for extraction statistics in logs
          if [ -f "ai_extractor.log" ]; then
            # Extract metrics from logs
            EXTRACTED_COUNT=$(grep -o "Successful extractions: [0-9]*" ai_extractor.log | tail -1 | grep -o "[0-9]*" || echo "0")
            QUALITY_SCORE=$(grep -o "Average quality: [0-9.]*" ai_extractor.log | tail -1 | grep -o "[0-9.]*" || echo "0")
          fi
          
          # Check database for actual results
          python -c "
          import os
          try:
              from supabase import create_client
              
              client = create_client(
                  os.getenv('SUPABASE_URL'),
                  os.getenv('SUPABASE_SERVICE_ROLE_KEY')
              )
              
              # Count recent extractions
              response = client.table('subsidies_structured').select('id').limit(1000).execute()
              total_structured = len(response.data)
              
              print(f'Database check: {total_structured} structured records found')
              
              with open('extraction_stats.txt', 'w') as f:
                  f.write(f'extracted_count={total_structured}\n')
                  
          except Exception as e:
              print(f'Database check failed: {e}')
              with open('extraction_stats.txt', 'w') as f:
                  f.write(f'extracted_count=0\n')
          " || echo "Database check script failed"
          
          # Read database results if available
          if [ -f "extraction_stats.txt" ]; then
            source extraction_stats.txt
            if [ "$extracted_count" -gt "$EXTRACTED_COUNT" ]; then
              EXTRACTED_COUNT=$extracted_count
            fi
          fi
          
          echo "üìä AI Extraction Results:"
          echo "  Extracted records: $EXTRACTED_COUNT"
          echo "  Average quality: $QUALITY_SCORE%"
          echo "  Exit code: $EXTRACTION_EXIT_CODE"
          
          # Set outputs
          echo "extracted=$EXTRACTED_COUNT" >> $GITHUB_OUTPUT
          echo "quality_score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          
          # Determine success
          if [ "$EXTRACTION_EXIT_CODE" -eq "0" ] && [ "$EXTRACTED_COUNT" -gt "0" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ AI extraction completed successfully"
          elif [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ AI extraction dry run completed"
          else
            echo "success=false" >> $GITHUB_OUTPUT
            echo "‚ùå AI extraction failed or no records extracted"
          fi

      - name: üìä Upload extraction artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-extraction-output-${{ github.run_number }}
          path: |
            AgriToolScraper-main/data/extracted/
            AgriToolScraper-main/debug/
            AgriToolScraper-main/logs/
            AgriToolScraper-main/*.log
            AgriToolScraper-main/*_stats.txt
            AgriToolScraper-main/*_stats.json
          retention-days: 14

  # =============================================================================
  # JOB 4: COMPREHENSIVE QUALITY ASSURANCE
  # =============================================================================
  comprehensive-qa:
    name: üîç Comprehensive Quality Assurance
    runs-on: ubuntu-latest
    needs: [setup-and-validate, enhanced-scraping, enhanced-ai-extraction]
    if: needs.enhanced-ai-extraction.result == 'success' && github.event.inputs.run_tests != 'false'
    timeout-minutes: 60
    outputs:
      qa-success: ${{ steps.quality-tests.outputs.success }}
      coverage-score: ${{ steps.quality-tests.outputs.coverage }}

    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      OPENAI_API_KEY: ${{ secrets.SCRAPPER_RAW_GPT_API }}
    
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install QA dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-cov psycopg2-binary
          
          # Install all project dependencies
          pip install -r AgriToolScraper-main/requirements.txt
          
          echo "‚úÖ QA dependencies installed"

      - name: üß™ Comprehensive test suite
        id: quality-tests
        run: |
          echo "üß™ Running comprehensive QA test suite..."
          
          cd AgriToolScraper-main
          
          # Test 1: Module import validation
          echo "üìã Test 1: Enhanced module imports"
          python -c "
          import sys
          sys.path.append('.')
          
          # Test enhanced modules
          try:
              from scraper.core import extract_single_page
              from scraper.supabase_uploader import SupabaseUploader, UploadConfig
              from scraper.ai_extractor import AIExtractor, ExtractionConfig
              from scraper.runner import ScrapingConfig
              from scraper.discovery import discover_subsidy_urls
              print('‚úÖ All enhanced modules import successfully')
          except ImportError as e:
              print(f'‚ùå Module import failed: {e}')
              sys.exit(1)
          "
          
          # Test 2: Enhanced database connectivity
          echo "üìã Test 2: Enhanced database operations"
          python -c "
          import asyncio
          from scraper.supabase_uploader import SupabaseUploader
          
          async def test_db():
              uploader = SupabaseUploader()
              await uploader.initialize()
              print('‚úÖ Enhanced Supabase connection successful')
          
          asyncio.run(test_db())
          "
          
          # Test 3: AI extractor with quality assessment
          echo "üìã Test 3: Enhanced AI extractor validation"
          python -c "
          import asyncio
          from scraper.ai_extractor import AIExtractor, ExtractionConfig
          
          async def test_extractor():
              config = ExtractionConfig(enable_quality_assessment=True)
              extractor = AIExtractor(config)
              await extractor.initialize()
              print('‚úÖ Enhanced AI extractor initialization successful')
          
          asyncio.run(test_extractor())
          "
          
          # Test 4: Data quality validation
          echo "üìã Test 4: Data quality and integrity validation"
          python -c "
          from scraper.supabase_uploader import SupabaseUploader
          
          try:
              uploader = SupabaseUploader()
              client = uploader.client
              
              # Check data integrity
              raw_response = client.table('raw_scraped_pages').select('id, status').limit(100).execute()
              structured_response = client.table('subsidies_structured').select('id, title, extraction_quality_score').limit(100).execute()
              
              raw_count = len(raw_response.data)
              structured_count = len(structured_response.data)
              
              # Calculate quality metrics
              quality_scores = [r.get('extraction_quality_score', 0) for r in structured_response.data if r.get('extraction_quality_score')]
              avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
              
              print(f'üìä Data Quality Metrics:')
              print(f'   Raw pages: {raw_count}')
              print(f'   Structured records: {structured_count}')
              print(f'   Average quality score: {avg_quality:.1f}%')
              
              # Write metrics for output
              with open('qa_metrics.txt', 'w') as f:
                  f.write(f'coverage={avg_quality}\n')
                  f.write(f'structured_count={structured_count}\n')
              
              print('‚úÖ Data quality validation completed')
              
          except Exception as e:
              print(f'‚ö†Ô∏è Data quality check failed: {e}')
              with open('qa_metrics.txt', 'w') as f:
                  f.write(f'coverage=0\n')
                  f.write(f'structured_count=0\n')
          "
          
          # Test 5: End-to-end pipeline validation
          echo "üìã Test 5: End-to-end pipeline validation"
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "false" ]; then
            python runner.py --mode test --max-workers 2 || echo "‚ö†Ô∏è End-to-end test completed with warnings"
          else
            echo "üß™ Skipping end-to-end test in dry run mode"
          fi
          
          # Read QA metrics
          COVERAGE_SCORE=0
          if [ -f "qa_metrics.txt" ]; then
            source qa_metrics.txt
            COVERAGE_SCORE=$coverage
          fi
          
          echo "üìä QA Results:"
          echo "  Coverage score: $COVERAGE_SCORE%"
          
          # Set outputs
          echo "coverage=$COVERAGE_SCORE" >> $GITHUB_OUTPUT
          
          # Determine QA success
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ QA dry run completed"
          elif (( $(echo "$COVERAGE_SCORE >= 70" | bc -l) )); then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Quality assurance passed"
          else
            echo "success=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Quality assurance completed with warnings"
          fi

      - name: üìä Generate comprehensive QA report
        run: |
          echo "üìä Generating comprehensive QA report..."
          cat > comprehensive_qa_report.json << EOF
          {
            "pipeline_execution": {
              "run_id": "${{ github.run_number }}",
              "timestamp": "$(date -u '+%Y-%m-%d %H:%M:%S UTC')",
              "trigger": "${{ github.event_name }}",
              "dry_run": "${{ github.event.inputs.dry_run || 'false' }}"
            },
            "scraping_results": {
              "success": "${{ needs.enhanced-scraping.outputs.scrape-success }}",
              "records_processed": "${{ needs.enhanced-scraping.outputs.records-processed }}",
              "urls_discovered": "${{ needs.enhanced-scraping.outputs.urls-discovered }}"
            },
            "ai_extraction_results": {
              "success": "${{ needs.enhanced-ai-extraction.outputs.extraction-success }}",
              "extracted_count": "${{ needs.enhanced-ai-extraction.outputs.extracted-count }}",
              "quality_score": "${{ needs.enhanced-ai-extraction.outputs.quality-score }}"
            },
            "quality_assurance": {
              "tests_executed": true,
              "coverage_score": "${{ steps.quality-tests.outputs.coverage }}",
              "overall_success": "${{ steps.quality-tests.outputs.success }}"
            },
            "performance_metrics": {
              "total_pipeline_duration_minutes": "$(( $(date +%s) - ${{ github.event.head_commit.timestamp && 'github.event.head_commit.timestamp' || '0' }} ))",
              "scraping_duration": "estimated from logs",
              "extraction_duration": "estimated from logs"
            },
            "recommendations": [
              "Review extraction quality scores for optimization opportunities",
              "Monitor pipeline performance trends",
              "Validate data completeness in structured records"
            ]
          }
          EOF
          
          echo "‚úÖ Comprehensive QA report generated"

      - name: üì¶ Upload comprehensive QA artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-qa-report-${{ github.run_number }}
          path: |
            comprehensive_qa_report.json
            AgriToolScraper-main/qa_metrics.txt
            AgriToolScraper-main/*.log
          retention-days: 30

  # =============================================================================
  # JOB 5: PERFORMANCE MONITORING & OPTIMIZATION
  # =============================================================================
  performance-monitoring:
    name: üìà Performance Monitoring
    runs-on: ubuntu-latest
    needs: [enhanced-scraping, enhanced-ai-extraction, comprehensive-qa]
    if: always() && github.event.inputs.enable_monitoring != 'false'
    timeout-minutes: 30

    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
    
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üìä Analyze pipeline performance
        run: |
          echo "üìà Analyzing pipeline performance..."
          
          # Install monitoring dependencies
          pip install supabase psutil matplotlib pandas
          
          python -c "
          import json
          import time
          from datetime import datetime, timedelta
          
          try:
              from supabase import create_client
              import os
              
              client = create_client(
                  os.getenv('SUPABASE_URL'),
                  os.getenv('SUPABASE_SERVICE_ROLE_KEY')
              )
              
              # Analyze recent pipeline performance
              end_time = datetime.now()
              start_time = end_time - timedelta(days=7)  # Last 7 days
              
              # Get recent raw pages
              raw_response = client.table('raw_scraped_pages').select('*').gte(
                  'created_at', start_time.isoformat()
              ).execute()
              
              # Get recent structured data
              structured_response = client.table('subsidies_structured').select('*').gte(
                  'created_at', start_time.isoformat()
              ).execute()
              
              # Calculate performance metrics
              raw_count = len(raw_response.data)
              structured_count = len(structured_response.data)
              conversion_rate = (structured_count / raw_count * 100) if raw_count > 0 else 0
              
              # Quality analysis
              quality_scores = []
              for record in structured_response.data:
                  if record.get('extraction_quality_score'):
                      quality_scores.append(record['extraction_quality_score'])
              
              avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
              
              # Performance report
              performance_report = {
                  'analysis_period': {
                      'start': start_time.isoformat(),
                      'end': end_time.isoformat(),
                      'days': 7
                  },
                  'pipeline_metrics': {
                      'raw_pages_scraped': raw_count,
                      'structured_records_created': structured_count,
                      'conversion_rate_percent': round(conversion_rate, 2),
                      'average_quality_score': round(avg_quality, 2)
                  },
                  'current_run': {
                      'run_id': '${{ github.run_number }}',
                      'scraping_success': '${{ needs.enhanced-scraping.outputs.scrape-success }}',
                      'extraction_success': '${{ needs.enhanced-ai-extraction.outputs.extraction-success }}',
                      'records_this_run': '${{ needs.enhanced-ai-extraction.outputs.extracted-count }}',
                      'quality_this_run': '${{ needs.enhanced-ai-extraction.outputs.quality-score }}'
                  },
                  'recommendations': []
              }
              
              # Generate recommendations
              if conversion_rate < 80:
                  performance_report['recommendations'].append('Low conversion rate detected - review extraction failures')
              
              if avg_quality < 70:
                  performance_report['recommendations'].append('Average quality below threshold - review extraction prompts')
              
              if raw_count > structured_count * 2:
                  performance_report['recommendations'].append('High backlog detected - consider increasing AI processing frequency')
              
              # Save performance report
              with open('performance_report.json', 'w') as f:
                  json.dump(performance_report, f, indent=2)
              
              print('üìä Performance Analysis Results:')
              print(f'   Raw pages (7 days): {raw_count}')
              print(f'   Structured records (7 days): {structured_count}')
              print(f'   Conversion rate: {conversion_rate:.1f}%')
              print(f'   Average quality: {avg_quality:.1f}%')
              print('‚úÖ Performance analysis completed')
              
          except Exception as e:
              print(f'‚ùå Performance analysis failed: {e}')
              
              # Create minimal report
              with open('performance_report.json', 'w') as f:
                  json.dump({
                      'error': str(e),
                      'timestamp': datetime.now().isoformat()
                  }, f, indent=2)
          "

      - name: üì¶ Upload performance monitoring artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-monitoring-${{ github.run_number }}
          path: |
            performance_report.json
          retention-days: 30

  # =============================================================================
  # JOB 6: INTELLIGENT CLEANUP & OPTIMIZATION
  # =============================================================================
  intelligent-cleanup:
    name: üßπ Intelligent Cleanup & Optimization
    runs-on: ubuntu-latest
    needs: [enhanced-scraping, enhanced-ai-extraction, comprehensive-qa, performance-monitoring]
    if: always()
    timeout-minutes: 20

    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
    
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üßπ Enhanced cleanup with optimization
        run: |
          echo "üßπ Starting intelligent cleanup process..."
          
          # Clean up temporary files with detailed logging
          echo "üóëÔ∏è Cleaning temporary files..."
          find . -name "*.tmp" -type f -delete 2>/dev/null || true
          find . -name "*.lock" -type f -delete 2>/dev/null || true
          find . -name ".pytest_cache" -type d -exec rm -rf {} + 2>/dev/null || true
          find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
          find . -name "*.pyc" -type f -delete 2>/dev/null || true
          
          # Clean up browser artifacts
          echo "üóëÔ∏è Cleaning browser artifacts..."
          pkill -f chrome 2>/dev/null || true
          pkill -f chromium 2>/dev/null || true
          pkill -f firefox 2>/dev/null || true
          pkill -f Xvfb 2>/dev/null || true
          
          # Clean up large log files (keep important ones)
          echo "üóëÔ∏è Optimizing log files..."
          find . -name "*.log" -size +10M -delete 2>/dev/null || true
          
          # Database optimization (if not dry run)
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "false" ]; then
            echo "üóëÔ∏è Database optimization..."
            pip install supabase 2>/dev/null || true
            
            python -c "
            import os
            from datetime import datetime, timedelta
            
            try:
                from supabase import create_client
                
                client = create_client(
                    os.getenv('SUPABASE_URL'),
                    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
                )
                
                # Clean up old failed records (older than 30 days)
                cutoff_date = (datetime.now() - timedelta(days=30)).isoformat()
                
                # Get count of old failed records
                old_failed = client.table('raw_scraped_pages').select('id').eq(
                    'status', 'failed'
                ).lt('created_at', cutoff_date).execute()
                
                if old_failed.data:
                    print(f'üóëÔ∏è Found {len(old_failed.data)} old failed records to clean')
                    # Could implement cleanup here if needed
                
                print('‚úÖ Database optimization completed')
                
            except Exception as e:
                print(f'‚ö†Ô∏è Database optimization skipped: {e}')
            " || echo "Database optimization script completed with warnings"
          fi
          
          echo "‚úÖ Intelligent cleanup completed successfully"

      - name: üìä Generate cleanup report
        run: |
          echo "üìä Cleanup Summary Report:"
          echo "  üóëÔ∏è Temporary files cleaned"
          echo "  üóëÔ∏è Cache directories cleared"
          echo "  üóëÔ∏è Browser processes terminated"
          echo "  üóëÔ∏è Large log files optimized"
          echo "  üóëÔ∏è Python bytecode cleaned"
          
          # Calculate disk space freed
          DISK_USAGE_AFTER=$(df -h . | tail -1 | awk '{print $4}')
          echo "  üíæ Available disk space: $DISK_USAGE_AFTER"
          echo "‚úÖ System optimized for next pipeline run"

  # =============================================================================
  # JOB 7: ENHANCED PIPELINE SUMMARY & NOTIFICATIONS
  # =============================================================================
  enhanced-pipeline-summary:
    name: üìã Enhanced Pipeline Summary
    runs-on: ubuntu-latest
    needs: [setup-and-validate, enhanced-scraping, enhanced-ai-extraction, comprehensive-qa, performance-monitoring, intelligent-cleanup]
    if: always()
    
    steps:
      - name: üìä Generate enhanced pipeline summary
        run: |
          echo "üìã === AGRITOOL ENHANCED PIPELINE EXECUTION SUMMARY ==="
          echo "üèÉ Run ID: ${{ github.run_number }}"
          echo "üìÖ Execution Date: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "‚öôÔ∏è Trigger: ${{ github.event_name }}"
          echo "üîß Mode: ${{ github.event.inputs.scraper_mode || 'scrape' }}"
          echo "üß™ Dry Run: ${{ github.event.inputs.dry_run || 'false' }}"
          echo ""
          echo "üìä DETAILED RESULTS:"
          echo "  üîß Environment Setup: ${{ needs.setup-and-validate.result }}"
          echo "  üï∑Ô∏è Enhanced Scraping: ${{ needs.enhanced-scraping.result }}"
          echo "    ‚îú‚îÄ Records Processed: ${{ needs.enhanced-scraping.outputs.records-processed || 'N/A' }}"
          echo "    ‚îî‚îÄ URLs Discovered: ${{ needs.enhanced-scraping.outputs.urls-discovered || 'N/A' }}"
          echo "  ü§ñ AI Extraction: ${{ needs.enhanced-ai-extraction.result }}"
          echo "    ‚îú‚îÄ Records Extracted: ${{ needs.enhanced-ai-extraction.outputs.extracted-count || 'N/A' }}"
          echo "    ‚îî‚îÄ Quality Score: ${{ needs.enhanced-ai-extraction.outputs.quality-score || 'N/A' }}%"
          echo "  üîç Quality Assurance: ${{ needs.comprehensive-qa.result }}"
          echo "    ‚îî‚îÄ Coverage Score: ${{ needs.comprehensive-qa.outputs.coverage-score || 'N/A' }}%"
          echo "  üìà Performance Monitoring: ${{ needs.performance-monitoring.result }}"
          echo "  üßπ Cleanup: ${{ needs.intelligent-cleanup.result }}"
          echo ""
          echo "üéØ OVERALL PIPELINE STATUS:"
          
          # Calculate overall success
          SETUP_SUCCESS="${{ needs.setup-and-validate.result }}"
          SCRAPE_SUCCESS="${{ needs.enhanced-scraping.result }}"
          AI_SUCCESS="${{ needs.enhanced-ai-extraction.result }}"
          QA_SUCCESS="${{ needs.comprehensive-qa.result }}"
          
          if [ "$SETUP_SUCCESS" = "success" ] && [ "$SCRAPE_SUCCESS" = "success" ] && [ "$AI_SUCCESS" = "success" ]; then
            echo "  ‚úÖ PIPELINE SUCCESS - All critical components completed successfully"
            echo "  üöÄ Enhanced AgriTool automation pipeline executed end-to-end"
            echo "  üìä Data processing pipeline operational and producing quality results"
            OVERALL_STATUS="SUCCESS"
          elif [ "$SCRAPE_SUCCESS" = "success" ] || [ "$AI_SUCCESS" = "success" ]; then
            echo "  ‚ö†Ô∏è PIPELINE PARTIAL SUCCESS - Core functionality working with some issues"
            echo "  üîß Review component logs for optimization opportunities"
            OVERALL_STATUS="PARTIAL_SUCCESS"
          else
            echo "  ‚ùå PIPELINE FAILURE - Critical components failed"
            echo "  üö® Immediate attention required - check logs and environment"
            OVERALL_STATUS="FAILURE"
          fi
          
          echo ""
          echo "üì¶ ENHANCED ARTIFACTS AVAILABLE:"
          echo "  üìÅ enhanced-scraper-output-${{ github.run_number }} (Scraping results and logs)"
          echo "  üìÅ ai-extraction-output-${{ github.run_number }} (AI processing results)"
          echo "  üìÅ comprehensive-qa-report-${{ github.run_number }} (Quality assurance metrics)"
          echo "  üìÅ performance-monitoring-${{ github.run_number }} (Performance analysis)"
          echo ""
          echo "üîó NEXT STEPS & RECOMMENDATIONS:"
          echo "  1. üìä Review comprehensive QA report for detailed metrics"
          echo "  2. üóÑÔ∏è Check Supabase subsidies_structured table for new records"
          echo "  3. üìà Monitor performance trends in the monitoring report"
          echo "  4. üîç Validate data completeness and extraction quality"
          echo "  5. ‚öôÔ∏è Consider pipeline optimizations based on performance data"
          
          # Save overall status for potential future use
          echo "PIPELINE_STATUS=$OVERALL_STATUS" >> $GITHUB_ENV

      - name: ‚úÖ Enhanced success notification
        if: |
          needs.setup-and-validate.result == 'success' && 
          needs.enhanced-scraping.result == 'success' && 
          needs.enhanced-ai-extraction.result == 'success'
        run: |
          echo "üéâ AGRITOOL ENHANCED PIPELINE COMPLETED SUCCESSFULLY!"
          echo "‚úÖ End-to-end automation executed with modern architecture"
          echo "üìä Performance Metrics:"
          echo "  ‚îú‚îÄ Scraped: ${{ needs.enhanced-scraping.outputs.records-processed || '0' }} records"
          echo "  ‚îú‚îÄ Extracted: ${{ needs.enhanced-ai-extraction.outputs.extracted-count || '0' }} records"
          echo "  ‚îú‚îÄ Quality: ${{ needs.enhanced-ai-extraction.outputs.quality-score || 'N/A' }}% average"
          echo "  ‚îî‚îÄ Coverage: ${{ needs.comprehensive-qa.outputs.coverage-score || 'N/A' }}%"
          echo ""
          echo "üöÄ AgriTool is now updated with fresh subsidy data!"

      - name: ‚ö†Ô∏è Partial success notification  
        if: |
          needs.enhanced-scraping.result == 'success' || 
          needs.enhanced-ai-extraction.result == 'success'
        run: |
          echo "‚ö†Ô∏è AGRITOOL PIPELINE COMPLETED WITH PARTIAL SUCCESS"
          echo "‚úÖ Some components succeeded, manual review recommended"
          echo "üìä Results Summary:"
          echo "  ‚îú‚îÄ Scraping: ${{ needs.enhanced-scraping.result }}"
          echo "  ‚îú‚îÄ AI Extraction: ${{ needs.enhanced-ai-extraction.result }}"
          echo "  ‚îî‚îÄ Quality Assurance: ${{ needs.comprehensive-qa.result }}"
          echo ""
          echo "üîß Check individual job logs and artifacts for optimization opportunities"

      - name: ‚ùå Failure notification
        if: |
          needs.setup-and-validate.result != 'success' ||
          (needs.enhanced-scraping.result != 'success' && needs.enhanced-ai-extraction.result != 'success')
        run: |
          echo "‚ùå AGRITOOL PIPELINE EXECUTION FAILED"
          echo "üö® Critical components failed - immediate attention required"
          echo "üìä Failure Analysis:"
          echo "  ‚îú‚îÄ Environment Setup: ${{ needs.setup-and-validate.result }}"
          echo "  ‚îú‚îÄ Scraping: ${{ needs.enhanced-scraping.result }}"
          echo "  ‚îú‚îÄ AI Extraction: ${{ needs.enhanced-ai-extraction.result }}"
          echo "  ‚îî‚îÄ Quality Assurance: ${{ needs.comprehensive-qa.result }}"
          echo ""
          echo "üîß Troubleshooting Steps:"
          echo "  1. Check environment variables and secrets configuration"
          echo "  2. Review individual job logs for specific error messages"
          echo "  3. Verify Supabase and OpenAI API connectivity"
          echo "  4. Check system resource availability and limits"
          echo "  5. Consider running individual components manually for debugging"

      - name: üì§ Pipeline completion webhook (optional)
        if: always()
        run: |
          # This step could send notifications to external systems
          # For now, we'll just log the completion
          echo "üì§ Pipeline execution completed"
          echo "üìã Status: ${{ env.PIPELINE_STATUS || 'UNKNOWN' }}"
          echo "üïê Completed at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          
          # Future: Send to Slack, Discord, or other notification systems
          # curl -X POST "$WEBHOOK_URL" -H 'Content-Type: application/json' \
          #   -d '{"text":"AgriTool Pipeline completed with status: ${{ env.PIPELINE_STATUS }}"}'

  # =============================================================================
  # JOB 8: AUTOMATED HEALTH CHECK (OPTIONAL)
  # =============================================================================
  automated-health-check:
    name: üè• Automated Health Check
    runs-on: ubuntu-latest
    needs: [enhanced-pipeline-summary]
    if: |
      always() && 
      github.event_name == 'schedule' &&
      (needs.enhanced-scraping.result == 'success' || needs.enhanced-ai-extraction.result == 'success')
    timeout-minutes: 15

    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
    
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üè• System health assessment
        run: |
          echo "üè• Performing automated health check..."
          
          pip install supabase pandas
          
          python -c "
          import json
          from datetime import datetime, timedelta
          
          try:
              from supabase import create_client
              import os
              
              client = create_client(
                  os.getenv('SUPABASE_URL'),
                  os.getenv('SUPABASE_SERVICE_ROLE_KEY')
              )
              
              health_report = {
                  'timestamp': datetime.now().isoformat(),
                  'pipeline_run_id': '${{ github.run_number }}',
                  'health_status': 'healthy',
                  'checks': {},
                  'alerts': []
              }
              
              # Check 1: Data freshness
              recent_cutoff = (datetime.now() - timedelta(hours=24)).isoformat()
              recent_raw = client.table('raw_scraped_pages').select('id').gte(
                  'created_at', recent_cutoff
              ).execute()
              
              # Get recent structured data
              recent_structured = client.table('subsidies_structured').select('id').gte(
                  'created_at', recent_cutoff
              ).execute()
              
              health_report['checks']['data_freshness'] = {
                  'raw_pages_24h': len(recent_raw.data),
                  'structured_records_24h': len(recent_structured.data),
                  'status': 'healthy' if len(recent_raw.data) > 0 else 'warning'
              }
              
              # Check 2: Quality trends
              quality_response = client.table('subsidies_structured').select(
                  'extraction_quality_score'
              ).gte('created_at', recent_cutoff).execute()
              
              quality_scores = [r['extraction_quality_score'] for r in quality_response.data 
                              if r.get('extraction_quality_score')]
              avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
              
              health_report['checks']['quality_trends'] = {
                  'average_quality_24h': round(avg_quality, 2),
                  'quality_samples': len(quality_scores),
                  'status': 'healthy' if avg_quality >= 70 else 'warning'
              }
              
              # Check 3: Error rates
              failed_raw = client.table('raw_scraped_pages').select('id').eq(
                  'status', 'failed'
              ).gte('created_at', recent_cutoff).execute()
              
              total_attempts = len(recent_raw.data) + len(failed_raw.data)
              error_rate = (len(failed_raw.data) / total_attempts * 100) if total_attempts > 0 else 0
              
              health_report['checks']['error_rates'] = {
                  'failed_extractions_24h': len(failed_raw.data),
                  'error_rate_percent': round(error_rate, 2),
                  'status': 'healthy' if error_rate < 20 else 'warning'
              }
              
              # Generate alerts based on health checks
              if health_report['checks']['data_freshness']['status'] == 'warning':
                  health_report['alerts'].append('No new data in last 24 hours')
              
              if health_report['checks']['quality_trends']['status'] == 'warning':
                  health_report['alerts'].append(f'Quality below threshold: {avg_quality:.1f}%')
              
              if health_report['checks']['error_rates']['status'] == 'warning':
                  health_report['alerts'].append(f'High error rate: {error_rate:.1f}%')
              
              # Overall health status
              warning_checks = sum(1 for check in health_report['checks'].values() 
                                 if check['status'] == 'warning')
              
              if warning_checks >= 2:
                  health_report['health_status'] = 'warning'
              elif warning_checks >= 3:
                  health_report['health_status'] = 'critical'
              
              # Save health report
              with open('health_report.json', 'w') as f:
                  json.dump(health_report, f, indent=2)
              
              print('üè• Health Check Results:')
              print(f'   Overall Status: {health_report[\"health_status\"].upper()}')
              print(f'   Data Freshness: {health_report[\"checks\"][\"data_freshness\"][\"status\"]}')
              print(f'   Quality Trends: {health_report[\"checks\"][\"quality_trends\"][\"status\"]}')
              print(f'   Error Rates: {health_report[\"checks\"][\"error_rates\"][\"status\"]}')
              
              if health_report['alerts']:
                  print(f'   Alerts: {len(health_report[\"alerts\"])}')
                  for alert in health_report['alerts']:
                      print(f'     - {alert}')
              
              print('‚úÖ Automated health check completed')
              
          except Exception as e:
              print(f'‚ùå Health check failed: {e}')
              with open('health_report.json', 'w') as f:
                  json.dump({
                      'health_status': 'unknown',
                      'error': str(e),
                      'timestamp': datetime.now().isoformat()
                  }, f, indent=2)
          "

      - name: üì¶ Upload health check artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: health-check-report-${{ github.run_number }}
          path: |
            health_report.json
          retention-days: 90

      - name: üéØ Final pipeline status
        run: |
          echo ""
          echo "üéØ === FINAL PIPELINE STATUS ==="
          echo ""
          
          # Determine final status based on critical components
          CRITICAL_SUCCESS=true
          
          if [ "${{ needs.enhanced-scraping.result }}" != "success" ] && [ "${{ needs.enhanced-ai-extraction.result }}" != "success" ]; then
            CRITICAL_SUCCESS=false
          fi
          
          if [ "$CRITICAL_SUCCESS" = "true" ]; then
            echo "üü¢ PIPELINE STATUS: SUCCESS"
            echo "‚úÖ AgriTool enhanced automation pipeline completed successfully"
            echo "üìä Fresh subsidy data is now available in the structured database"
            echo "üöÄ System is ready for user queries and applications"
          else
            echo "üî¥ PIPELINE STATUS: FAILURE"
            echo "‚ùå Critical components failed - system may have stale data"
            echo "üîß Manual intervention required to restore full functionality"
          fi
          
          echo ""
          echo "üìà MONITORING & MAINTENANCE:"
          echo "  üîç Monitor the health check reports for system trends"
          echo "  üìä Review performance metrics for optimization opportunities"
          echo "  üîÑ Next automated run scheduled based on cron settings"
          echo "  üìß Check GitHub Actions for any configuration updates needed"
          echo ""
          echo "üèÅ Enhanced AgriTool Pipeline Execution Complete"
