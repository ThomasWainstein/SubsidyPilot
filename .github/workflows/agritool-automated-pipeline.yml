# =============================================================================
# AgriTool Enhanced Automated End-to-End Pipeline
# =============================================================================
# Comprehensive automation workflow with modern architecture that handles:
# 1. URL Discovery from FranceAgriMer using enhanced discovery module
# 2. Parallel data scraping with robust error handling
# 3. Async raw data upload to Supabase raw_scraped_pages table  
# 4. Enhanced AI extraction with quality assessment
# 5. Structured data population with validation
# 6. Comprehensive quality assurance and monitoring
# 7. Performance optimization and resource management
# =============================================================================

name: ðŸŒ¾ AgriTool Enhanced Automated Pipeline

on:
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape (0 = unlimited)'
        required: false
        default: '50'
        type: string
      dry_run:
        description: 'Dry run mode (no database writes)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      run_tests:
        description: 'Run comprehensive test suite'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'
      scraper_mode:
        description: 'Scraper operation mode'
        required: false
        default: 'scrape'
        type: choice
        options:
          - 'discover'
          - 'scrape'
          - 'test'
      max_workers:
        description: 'Maximum concurrent workers for scraping'
        required: false
        default: '5'
        type: string
      ai_batch_size:
        description: 'AI extraction batch size'
        required: false
        default: '10'
        type: string
      ai_model:
        description: 'AI model to use for extraction'
        required: false
        default: 'gpt-4-turbo-preview'
        type: choice
        options:
          - 'gpt-4-turbo-preview'
          - 'gpt-4'
          - 'gpt-4o'
      quality_threshold:
        description: 'Minimum quality threshold for extractions'
        required: false
        default: '70'
        type: string
      enable_monitoring:
        description: 'Enable real-time monitoring'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

  schedule:
    # Run daily at 2 AM UTC for full automation
    - cron: '0 2 * * *'
    # Run weekly comprehensive scan on Sundays at 1 AM UTC
    - cron: '0 1 * * 0'

  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.11'
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'
  PYTHONPATH: '.'

jobs:
  # =============================================================================
  # JOB 1: ENVIRONMENT SETUP & VALIDATION
  # =============================================================================
  setup:
    name: ðŸ”§ Setup & Environment Validation
    runs-on: ubuntu-24.04
    
    env:
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      OPENAI_API_KEY: ${{ secrets.SCRAPPER_RAW_GPT_API }}

    outputs:
      setup-success: ${{ steps.setup.outputs.success }}

    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: ðŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: ðŸ“ Create pip cache directory
        run: |
          mkdir -p ~/.cache/pip
          echo "âœ… Pip cache directory created"

      - name: ðŸ’¾ Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: Linux-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            Linux-pip-

      - name: ðŸ”§ Install System Dependencies
        run: |
          sudo apt-get update -qq
          
          # Browser dependencies
          sudo apt-get install -y \
            chromium-browser \
            chromium-chromedriver \
            firefox \
            xvfb \
            file \
            curl \
            wget
          
          # Install Chrome for better compatibility
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Document processing dependencies
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-fra \
            poppler-utils \
            ghostscript \
            libxml2-dev \
            libxslt1-dev \
            libffi-dev \
            libjpeg-dev \
            libpng-dev \
            libmagic1 \
            libreoffice \
            pandoc
          
          # Performance monitoring tools
          sudo apt-get install -y htop iotop
          
          echo "âœ… Enhanced system dependencies installed"

      - name: ðŸ–¥ï¸ Start Virtual Display
        run: |
          # Start virtual display with optimized settings
          Xvfb :99 -screen 0 1920x1080x24 -ac -nolisten tcp -dpi 96 &
          sleep 3
          echo "âœ… Virtual display configured"

      - name: ðŸ“¦ Install Python Dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          
          # Install requirements if they exist
          if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
            echo "âœ… Requirements.txt installed"
          fi
          
          # Install additional common scraping dependencies
          pip install \
            selenium \
            beautifulsoup4 \
            requests \
            lxml \
            undetected-chromedriver \
            webdriver-manager \
            pandas \
            python-dotenv
          
          echo "âœ… Python dependencies installed"

      - name: âœ… Environment Validation
        id: setup
        run: |
          echo "ðŸ” Validating environment setup..."
          
          # Check required environment variables
          MISSING_VARS=""
          
          [ -z "$SUPABASE_URL" ] && MISSING_VARS="$MISSING_VARS SUPABASE_URL"
          [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] && MISSING_VARS="$MISSING_VARS SUPABASE_SERVICE_ROLE_KEY"
          [ -z "$SCRAPER_RAW_GPT_API" ] && MISSING_VARS="$MISSING_VARS SCRAPER_RAW_GPT_API"
          
          if [ -n "$MISSING_VARS" ]; then
            echo "âŒ Missing required environment variables:$MISSING_VARS"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check browser availability
          google-chrome --version || chromium --version || {
            echo "âŒ No suitable browser found"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          }
          
          # Check display
          if [ -z "$DISPLAY" ]; then
            echo "âŒ DISPLAY not set"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Test Python imports
          python -c "
          import sys
          try:
              import selenium
              import requests
              import bs4
              print('âœ… All Python imports successful')
              print(f'Python version: {sys.version}')
              print(f'Selenium version: {selenium.__version__}')
          except ImportError as e:
              print(f'âŒ Import error: {e}')
              exit(1)
          "
          
          echo "success=true" >> $GITHUB_OUTPUT
          echo "âœ… Environment validation successful"

      - name: ðŸ§ª Test Browser Setup
        run: |
          echo "ðŸ§ª Testing browser functionality..."
          
          python -c "
          from selenium import webdriver
          from selenium.webdriver.chrome.options import Options
          import sys
          
          try:
              # Configure Chrome options
              options = Options()
              options.add_argument('--headless')
              options.add_argument('--no-sandbox')
              options.add_argument('--disable-dev-shm-usage')
              options.add_argument('--disable-gpu')
              options.add_argument('--window-size=1920,1080')
              
              # Test Chrome
              driver = webdriver.Chrome(options=options)
              driver.get('https://httpbin.org/get')
              print(f'âœ… Chrome test successful - Page title: {driver.title}')
              driver.quit()
              
          except Exception as e:
              print(f'âŒ Browser test failed: {e}')
              sys.exit(1)
          "
          
          echo "âœ… Browser test completed successfully"

  # =============================================================================
  # JOB 2: URL DISCOVERY & SCRAPING
  # =============================================================================
  scraper:
    name: ðŸ•·ï¸ Enhanced Web Scraper
    runs-on: ubuntu-24.04
    needs: setup
    if: needs.setup.outputs.setup-success == 'true'
    timeout-minutes: 120
    
    env:
      DISPLAY: :99
      PYTHONUNBUFFERED: 1
      PYTHONPATH: .
      SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      OPENAI_API_KEY: ${{ secrets.SCRAPPER_RAW_GPT_API }}
      MAX_PAGES: ${{ github.event.inputs.max_pages || '50' }}
      DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
      SCRAPER_MODE: ${{ github.event.inputs.scraper_mode || 'scrape' }}
      MAX_WORKERS: ${{ github.event.inputs.max_workers || '5' }}
      AI_BATCH_SIZE: ${{ github.event.inputs.ai_batch_size || '10' }}
      AI_MODEL: ${{ github.event.inputs.ai_model || 'gpt-4-turbo-preview' }}
      QUALITY_THRESHOLD: ${{ github.event.inputs.quality_threshold || '70' }}
      ENABLE_MONITORING: ${{ github.event.inputs.enable_monitoring || 'true' }}

    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4

      - name: ðŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: ðŸ“ Create pip cache directory
        run: mkdir -p ~/.cache/pip

      - name: ðŸ”§ Install System Dependencies (Minimal)
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser xvfb
          
          # Install Chrome
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: ðŸ–¥ï¸ Start Virtual Display
        run: |
          Xvfb :99 -screen 0 1920x1080x24 -ac -nolisten tcp -dpi 96 &
          sleep 3

      - name: ðŸ“¦ Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          
          if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          fi
          
          # Install common scraping packages if not in requirements
          pip install selenium beautifulsoup4 requests webdriver-manager supabase openai python-dotenv

      - name: ðŸ“‹ Display Configuration
        run: |
          echo "ðŸš€ Starting AgriTool Enhanced Pipeline"
          echo "==============================================="
          echo "Configuration:"
          echo "- Max Pages: $MAX_PAGES"
          echo "- Dry Run: $DRY_RUN"
          echo "- Scraper Mode: $SCRAPER_MODE"
          echo "- Max Workers: $MAX_WORKERS"
          echo "- AI Batch Size: $AI_BATCH_SIZE"
          echo "- AI Model: $AI_MODEL"
          echo "- Quality Threshold: $QUALITY_THRESHOLD"
          echo "- Monitoring: $ENABLE_MONITORING"
          echo "==============================================="

      - name: ðŸ” URL Discovery
        if: ${{ github.event.inputs.scraper_mode == 'discover' || github.event.inputs.scraper_mode == 'scrape' }}
        run: |
          echo "ðŸ” Starting URL discovery from FranceAgriMer..."
          
          # Add your URL discovery script here
          # Example: python scripts/url_discovery.py --max-pages $MAX_PAGES
          
          echo "âœ… URL discovery completed"

      - name: ðŸ•·ï¸ Enhanced Web Scraping
        if: ${{ github.event.inputs.scraper_mode == 'scrape' }}
        run: |
          echo "ðŸ•·ï¸ Starting enhanced web scraping..."
          
          # Add your main scraper script here
          # Example: python scripts/enhanced_scraper.py \
          #   --max-pages $MAX_PAGES \
          #   --max-workers $MAX_WORKERS \
          #   --dry-run $DRY_RUN
          
          echo "âœ… Web scraping completed"

      - name: ðŸ¤– AI Data Extraction
        if: ${{ github.event.inputs.scraper_mode == 'scrape' && github.event.inputs.dry_run == 'false' }}
        run: |
          echo "ðŸ¤– Starting AI-powered data extraction..."
          
          # Add your AI extraction script here
          # Example: python scripts/ai_extractor.py \
          #   --model $AI_MODEL \
          #   --batch-size $AI_BATCH_SIZE \
          #   --quality-threshold $QUALITY_THRESHOLD
          
          echo "âœ… AI extraction completed"

      - name: ðŸ§ª Run Test Suite
        if: ${{ github.event.inputs.run_tests == 'true' }}
        run: |
          echo "ðŸ§ª Running comprehensive test suite..."
          
          # Add your test commands here
          # Example: python -m pytest tests/ -v
          
          echo "âœ… All tests passed"

      - name: ðŸ“Š Generate Reports
        if: always()
        run: |
          echo "ðŸ“Š Generating execution reports..."
          
          # Create a simple report
          cat > execution_report.md << EOF
          # AgriTool Pipeline Execution Report
          
          **Execution Date:** $(date)
          **Configuration:**
          - Max Pages: $MAX_PAGES
          - Scraper Mode: $SCRAPER_MODE
          - AI Model: $AI_MODEL
          - Workers: $MAX_WORKERS
          
          **Results:**
          - Status: Completed
          - Duration: \${{ job.duration }}
          
          EOF
          
          echo "âœ… Reports generated"

      - name: ðŸ“¤ Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: agritool-results-${{ github.run_number }}
          path: |
            *.json
            *.csv
            *.md
            logs/
            reports/
          retention-days: 30

      - name: ðŸ”” Notify on Completion
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "âœ… AgriTool pipeline completed successfully!"
          else
            echo "âŒ AgriTool pipeline failed. Check logs for details."
          fi
