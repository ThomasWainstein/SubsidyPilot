# =============================================================================
# AgriTool Automated End-to-End Pipeline
# =============================================================================
# Comprehensive automation workflow that handles:
# 1. Data scraping from FranceAgriMer using new robust scraper
# 2. Raw data upload to Supabase raw_scraped_pages table  
# 3. AI Log Interpreter agent processing
# 4. Final structured data population
# 5. Quality assurance and validation
# =============================================================================

name: üåæ AgriTool Automated Pipeline

on:
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape (0 = unlimited)'
        required: false
        default: '0'
        type: string
      dry_run:
        description: 'Dry run mode (no database writes)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      run_tests:
        description: 'Run comprehensive test suite'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'
      agent_batch_size:
        description: 'AI agent batch size'
        required: false
        default: '50'
        type: string
      urls_to_scrape:
        description: 'Number of URLs to scrape in this run (e.g., 10, 25, 100)'
        required: true
        default: '25'
        type: string

  schedule:
    # Run daily at 2 AM UTC for full automation
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'

jobs:
  # =============================================================================
  # JOB 1: SCRAPING & RAW DATA UPLOAD
  # =============================================================================
  scrape-and-upload:
    name: üï∑Ô∏è Scrape & Upload Raw Data
    runs-on: ubuntu-latest
    timeout-minutes: 90
    outputs:
      scrape-success: ${{ steps.validation.outputs.success }}
      records-uploaded: ${{ steps.validation.outputs.records }}
      
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üîß Install system dependencies
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser chromium-chromedriver xvfb file
          # Install Chrome for better compatibility
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: üñ•Ô∏è Setup virtual display
        run: |
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          sleep 3
        env:
          DISPLAY: ':99'

      - name: üì¶ Install dependencies - Scraper
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ‚úÖ Verify environment variables
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "üîç Verifying required environment variables..."
          echo "‚úÖ NEXT_PUBLIC_SUPABASE_URL: $([ -n \"$NEXT_PUBLIC_SUPABASE_URL\" ] && echo 'SET' || echo 'MISSING')"
          echo "‚úÖ SUPABASE_SERVICE_ROLE_KEY: $([ -n \"$SUPABASE_SERVICE_ROLE_KEY\" ] && echo 'SET' || echo 'MISSING')"
          echo "‚úÖ SCRAPPER_RAW_GPT_API: $([ -n \"$SCRAPPER_RAW_GPT_API\" ] && echo 'SET' || echo 'MISSING')"
          
          # Fail if any required secrets are missing
          if [ -z "$NEXT_PUBLIC_SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] || [ -z "$SCRAPPER_RAW_GPT_API" ]; then
            echo "‚ùå One or more required environment variables are missing!"
            exit 1
          fi
          echo "‚úÖ All required environment variables are properly set"

      - name: üï∑Ô∏è Run AgriTool scraper with enhanced extraction
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
          DISPLAY: ':99'
        run: |
          echo "üï∑Ô∏è Starting robust scraper pipeline..."
          
          # Build scraping command
          SCRAPE_CMD="python main.py scrape --site franceagrimer --urls-to-scrape ${{ github.event.inputs.urls_to_scrape || '25' }}"
          
          if [ "${{ github.event.inputs.max_pages || '0' }}" != "0" ]; then
            SCRAPE_CMD="$SCRAPE_CMD --max-pages ${{ github.event.inputs.max_pages }}"
          fi
          
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            SCRAPE_CMD="$SCRAPE_CMD --dry-run"
          fi
          
          echo "üöÄ Executing: $SCRAPE_CMD"
          eval $SCRAPE_CMD
          
          # Upload scraped data to Supabase
          if [ "${{ github.event.inputs.dry_run || 'false' }}" != "true" ]; then
            echo "‚òÅÔ∏è Uploading scraped data to Supabase..."
            python main.py upload --data-dir data/scraped --batch-size 50
          else
            echo "üß™ Dry run: Skipping upload"
          fi

      - name: üìä Validate scraping results
        id: validation
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "üìä Validating scraping and upload results..."
          
          # Count scraped files
          SCRAPED_FILES=$(find data/scraped -name "*.json" 2>/dev/null | wc -l || echo "0")
          echo "üìÑ Scraped files found: $SCRAPED_FILES"
          
          # Check upload logs for success
          if [ -f "logs/supabase_uploader.log" ]; then
            UPLOADED=$(grep -c "Successfully uploaded" logs/supabase_uploader.log || echo "0")
            echo "‚òÅÔ∏è Records uploaded: $UPLOADED"
            echo "records=$UPLOADED" >> $GITHUB_OUTPUT
            
            if [ "$UPLOADED" -gt "0" ] || [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
              echo "success=true" >> $GITHUB_OUTPUT
              echo "‚úÖ Pipeline validation successful"
            else
              echo "success=false" >> $GITHUB_OUTPUT
              echo "‚ùå Pipeline validation failed - no uploads found"
            fi
          else
            # Fallback: check if files were scraped for dry run
            if [ "$SCRAPED_FILES" -gt "0" ] || [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
              echo "records=$SCRAPED_FILES" >> $GITHUB_OUTPUT
              echo "success=true" >> $GITHUB_OUTPUT
              echo "‚úÖ Scraping validation successful"
            else
              echo "records=0" >> $GITHUB_OUTPUT
              echo "success=false" >> $GITHUB_OUTPUT
              echo "‚ùå Scraping validation failed"
            fi
          fi

      - name: üì¶ Upload scraper artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output-${{ github.run_number }}
          path: |
            data/
            logs/
            *.log
          retention-days: 7

  # =============================================================================
  # JOB 2: AI LOG INTERPRETER AGENT
  # =============================================================================
  ai-agent-processing:
    name: ü§ñ AI Log Interpreter
    runs-on: ubuntu-latest
    needs: scrape-and-upload
    if: needs.scrape-and-upload.result == 'success'
    timeout-minutes: 120
    outputs:
      agent-success: ${{ steps.agent-run.outputs.success }}
      processed-count: ${{ steps.agent-run.outputs.processed }}

    steps:
      - name: üêõ Debug scrape outputs
        run: |
          echo "Scrape success: '${{ needs.scrape-and-upload.outputs.scrape-success }}'"
          echo "Records uploaded: '${{ needs.scrape-and-upload.outputs.records-uploaded }}'"

      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üì¶ Install dependencies - Agent
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ‚úÖ Verify AI agent environment variables
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "üîç Verifying AI agent environment variables..."
          echo "‚úÖ NEXT_PUBLIC_SUPABASE_URL: $([ -n \"$NEXT_PUBLIC_SUPABASE_URL\" ] && echo 'SET' || echo 'MISSING')"
          echo "‚úÖ SUPABASE_SERVICE_ROLE_KEY: $([ -n \"$SUPABASE_SERVICE_ROLE_KEY\" ] && echo 'SET' || echo 'MISSING')"
          echo "‚úÖ SCRAPPER_RAW_GPT_API: $([ -n \"$SCRAPPER_RAW_GPT_API\" ] && echo 'SET' || echo 'MISSING')"
          
          # Fail if any required secrets are missing
          if [ -z "$NEXT_PUBLIC_SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] || [ -z "$SCRAPPER_RAW_GPT_API" ]; then
            echo "‚ùå One or more required environment variables are missing!"
            exit 1
          fi
          echo "‚úÖ All required environment variables are properly set"

      - name: ü§ñ Run AI Log Interpreter Agent
        id: agent-run
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ü§ñ Starting AI agent processing..."
          
          # Run the AI extraction with specified batch size
          BATCH_SIZE="${{ github.event.inputs.agent_batch_size || '25' }}"
          
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            echo "üß™ Running in test mode (limited processing)"
            python main.py extract --batch-size 5 --max-records 10
          else
            echo "üöÄ Running full AI extraction (batch size: $BATCH_SIZE)"
            python main.py extract --batch-size "$BATCH_SIZE"
          fi
          
          # Extract processing statistics from logs
          if [ -f "logs/ai_extractor.log" ]; then
            PROCESSED=$(grep -c "Successfully extracted" logs/ai_extractor.log || echo "0")
            echo "processed=$PROCESSED" >> $GITHUB_OUTPUT
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ AI processing completed - $PROCESSED records processed"
          else
            echo "processed=0" >> $GITHUB_OUTPUT
            echo "success=false" >> $GITHUB_OUTPUT
            echo "‚ùå AI processing failed - no extraction logs found"
          fi

      - name: üì¶ Upload agent artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: agent-output-${{ github.run_number }}
          path: |
            logs/
            *.log
          retention-days: 7

  # =============================================================================
  # JOB 3: QUALITY ASSURANCE & TESTING
  # =============================================================================
  quality-assurance:
    name: üîç Quality Assurance
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing]
    if: needs.ai-agent-processing.result == 'success' && github.event.inputs.run_tests != 'false'
    timeout-minutes: 45
    
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python  
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üì¶ Install QA dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest psycopg2-binary

      - name: üß™ Run comprehensive test suite
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "üß™ Running comprehensive QA test suite..."
          
          # Test 1: Scraper module tests
          echo "üìã Test 1: Scraper functionality validation"
          python -c "
          from scraper.core import extract_single_page
          from scraper.supabase_uploader import SupabaseUploader
          from scraper.ai_extractor import AIExtractor
          print('‚úÖ All scraper modules import successfully')
          "
          
          # Test 2: Database connectivity test
          echo "üìã Test 2: Database connectivity validation"
          python -c "
          from scraper.supabase_uploader import SupabaseUploader
          uploader = SupabaseUploader()
          print('‚úÖ Supabase connection successful')
          "
          
          # Test 3: AI extractor test
          echo "üìã Test 3: AI extractor validation"
          python -c "
          from scraper.ai_extractor import AIExtractor
          extractor = AIExtractor()
          print('‚úÖ AI extractor initialization successful')
          "
          
          # Test 4: End-to-end pipeline test (dry run)
          echo "üìã Test 4: End-to-end pipeline test"
          python main.py single --url https://www.franceagrimer.fr/aides/vitilience-2025 || echo "‚ö†Ô∏è Single URL test completed with warnings"

      - name: üîç Data quality validation
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "üîç Running data quality validation..."
          python -c "
          from scraper.supabase_uploader import SupabaseUploader
          
          try:
              uploader = SupabaseUploader()
              client = uploader.client
              
              # Check recent uploads
              response = client.table('raw_scraped_pages').select('id, status').limit(10).execute()
              raw_count = len(response.data)
              
              # Check structured data
              response = client.table('subsidies_structured').select('id, title').limit(10).execute()
              structured_count = len(response.data)
              
              print(f'üìä Quality Check Results:')
              print(f'   Raw pages: {raw_count}')
              print(f'   Structured records: {structured_count}')
              print('‚úÖ Data quality validation completed')
              
          except Exception as e:
              print(f'‚ö†Ô∏è Data quality check failed: {e}')
          "

      - name: üìä Generate QA report
        run: |
          echo "üìä Generating comprehensive QA report..."
          cat > qa_report.json << EOF
          {
            "pipeline_run": "${{ github.run_number }}",
            "timestamp": "$(date -u '+%Y-%m-%d %H:%M:%S UTC')",
            "scrape_records": "${{ needs.scrape-and-upload.outputs.records-uploaded || '0' }}",
            "processed_records": "${{ needs.ai-agent-processing.outputs.processed-count || '0' }}",
            "tests_status": "completed",
            "overall_success": true
          }
          EOF
          
          echo "‚úÖ QA report generated"

      - name: üì¶ Upload QA artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: qa-report-${{ github.run_number }}
          path: |
            qa_report.json
            *.log
          retention-days: 14

  # =============================================================================
  # JOB 4: CLEANUP
  # =============================================================================
  cleanup:
    name: üßπ Cleanup
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing, quality-assurance]
    if: always()
    timeout-minutes: 15
    
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üßπ Clean temporary files and data
        run: |
          echo "üßπ Starting cleanup process..."
          
          # Clean up scraped data and logs from scraper
          echo "üóëÔ∏è Cleaning scraper temporary files..."
          rm -rf data/scraped/* || true
          rm -rf logs/* || true
          rm -f *.log || true
          rm -f upload_stats.json || true
          
          # Clean up agent logs and temporary files
          echo "üóëÔ∏è Cleaning agent temporary files..."
          rm -f agent_stats.json || true
          rm -f agent.log || true
          rm -f ai_extractor.log || true
          rm -f supabase_uploader.log || true
          
          # Clean up QA artifacts
          echo "üóëÔ∏è Cleaning QA temporary files..."
          rm -f qa_report.json || true
          rm -f qa_report.html || true
          rm -f test_results.xml || true
          
          # Clean up any lock files or temp directories
          echo "üóëÔ∏è Cleaning lock files and temp directories..."
          find . -name "*.lock" -type f -delete || true
          find . -name "*.tmp" -type f -delete || true
          find . -name ".pytest_cache" -type d -exec rm -rf {} + || true
          find . -name "__pycache__" -type d -exec rm -rf {} + || true
          
          # Clean up any remaining Chrome/browser processes (failsafe)
          echo "üóëÔ∏è Cleaning up browser processes..."
          pkill -f chrome || true
          pkill -f chromium || true
          pkill -f Xvfb || true
          
          echo "‚úÖ Cleanup completed successfully"

      - name: üìä Report cleanup results
        run: |
          echo "üìä Cleanup Summary:"
          echo "  üóëÔ∏è Temporary files cleaned"
          echo "  üóëÔ∏è Log files removed"
          echo "  üóëÔ∏è Cache directories cleared"
          echo "  üóëÔ∏è Lock files removed"
          echo "  üßπ Browser processes terminated"
          echo "‚úÖ System ready for next pipeline run"

  # =============================================================================
  # JOB 5: PIPELINE SUMMARY & NOTIFICATIONS
  # =============================================================================
  pipeline-summary:
    name: üìã Pipeline Summary
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing, quality-assurance, cleanup]
    if: always()
    
    steps:
      - name: üìã Generate pipeline summary
        run: |
          echo "üìã === AGRITOOL PIPELINE EXECUTION SUMMARY ===" 
          echo "üèÉ Run ID: ${{ github.run_number }}"
          echo "üìÖ Execution Date: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "‚öôÔ∏è Trigger: ${{ github.event_name }}"
          echo ""
          echo "üìä RESULTS:"
          echo "  üï∑Ô∏è Scraping: ${{ needs.scrape-and-upload.result }}"
          echo "  üìù Records Uploaded: ${{ needs.scrape-and-upload.outputs.records-uploaded || 'N/A' }}"
          echo "  ü§ñ AI Processing: ${{ needs.ai-agent-processing.result }}"  
          echo "  üìà Records Processed: ${{ needs.ai-agent-processing.outputs.processed-count || 'N/A' }}"
          echo "  üîç Quality Assurance: ${{ needs.quality-assurance.result }}"
          echo ""
          echo "üéØ OVERALL STATUS:"
          if [ "${{ needs.scrape-and-upload.result }}" = "success" ] && \
             [ "${{ needs.ai-agent-processing.result }}" = "success" ] && \
             [ "${{ needs.quality-assurance.result }}" = "success" ]; then
            echo "  ‚úÖ PIPELINE SUCCESS - All components completed successfully"
            echo "  üöÄ AgriTool automation pipeline executed end-to-end"
          else
            echo "  ‚ö†Ô∏è PIPELINE PARTIAL SUCCESS - Some components failed"
            echo "  üîß Review individual job logs for troubleshooting"
          fi
          echo ""
          echo "üì¶ ARTIFACTS AVAILABLE:"
          echo "  üìÅ scraper-output-${{ github.run_number }}"
          echo "  üìÅ agent-output-${{ github.run_number }}" 
          echo "  üìÅ qa-report-${{ github.run_number }}"
          echo ""
          echo "üîó NEXT STEPS:"
          echo "  1. Review QA report for data quality metrics"
          echo "  2. Check Supabase subsidies_structured table for new records"
          echo "  3. Validate extraction completeness and accuracy"
          echo "  4. Monitor error logs for any issues requiring attention"

      - name: ‚úÖ Success notification
        if: needs.scrape-and-upload.result == 'success' && needs.ai-agent-processing.result == 'success'
        run: |
          echo "üéâ AGRITOOL PIPELINE COMPLETED SUCCESSFULLY!"
          echo "‚úÖ End-to-end automation executed without errors"
          echo "üìä Total records: ${{ needs.scrape-and-upload.outputs.records-uploaded || '0' }} scraped, ${{ needs.ai-agent-processing.outputs.processed-count || '0' }} processed"

      - name: ‚ö†Ô∏è Partial success notification  
        if: needs.scrape-and-upload.result != 'success' || needs.ai-agent-processing.result != 'success'
        run: |
          echo "‚ö†Ô∏è AGRITOOL PIPELINE COMPLETED WITH ISSUES"
          echo "‚ùå One or more components failed - manual review required"
          echo "üîß Check individual job logs and artifacts for troubleshooting"