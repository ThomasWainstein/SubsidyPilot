# =============================================================================
# AgriTool Automated End-to-End Pipeline
# =============================================================================
# Comprehensive automation workflow that handles:
# 1. Data scraping from FranceAgriMer using new robust scraper
# 2. Raw data upload to Supabase raw_scraped_pages table  
# 3. AI Log Interpreter agent processing
# 4. Final structured data population
# 5. Quality assurance and validation
# =============================================================================

name: ðŸŒ¾ AgriTool Automated Pipeline

on:
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape (0 = unlimited)'
        required: false
        default: '0'
        type: string
      dry_run:
        description: 'Dry run mode (no database writes)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      run_tests:
        description: 'Run comprehensive test suite'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'
      agent_batch_size:
        description: 'AI agent batch size'
        required: false
        default: '50'
        type: string
      urls_to_scrape:
        description: 'Number of URLs to scrape in this run (e.g., 10, 25, 100)'
        required: true
        default: '25'
        type: string

  schedule:
    # Run daily at 2 AM UTC for full automation
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'

jobs:
  # =============================================================================
  # JOB 1: SCRAPING & RAW DATA UPLOAD
  # =============================================================================
  scrape-and-upload:
    name: ðŸ•·ï¸ Scrape & Upload Raw Data
    runs-on: ubuntu-latest
    timeout-minutes: 90
    outputs:
      scrape-success: ${{ steps.validation.outputs.success }}
      records-uploaded: ${{ steps.validation.outputs.records }}
      
    steps:
      - name: ðŸ“‹ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ðŸ”§ Install system dependencies
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser chromium-chromedriver xvfb file
          # Install Chrome for better compatibility
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: ðŸ–¥ï¸ Setup virtual display
        run: |
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          sleep 3
        env:
          DISPLAY: ':99'

      - name: ðŸ“ Create required directories
        working-directory: ./AgriToolScraper-main
        run: |
          mkdir -p data/scraped
          mkdir -p logs

      - name: ðŸ“¦ Install dependencies - Scraper
        working-directory: ./AgriToolScraper-main  
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: âœ… Verify environment variables
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ðŸ” Verifying required environment variables..."
          echo "âœ… NEXT_PUBLIC_SUPABASE_URL: $([ -n \"$NEXT_PUBLIC_SUPABASE_URL\" ] && echo 'SET' || echo 'MISSING')"
          echo "âœ… SUPABASE_SERVICE_ROLE_KEY: $([ -n \"$SUPABASE_SERVICE_ROLE_KEY\" ] && echo 'SET' || echo 'MISSING')"
          echo "âœ… SCRAPPER_RAW_GPT_API: $([ -n \"$SCRAPPER_RAW_GPT_API\" ] && echo 'SET' || echo 'MISSING')"
          
          # Fail if any required secrets are missing
          if [ -z "$NEXT_PUBLIC_SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] || [ -z "$SCRAPPER_RAW_GPT_API" ]; then
            echo "âŒ One or more required environment variables are missing!"
            exit 1
          fi
          echo "âœ… All required environment variables are properly set"

      - name: ðŸ•·ï¸ Run AgriTool scraper with enhanced extraction
        working-directory: ./AgriToolScraper-main
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
          DISPLAY: ':99'
        run: |
          echo "ðŸ•·ï¸ Starting robust scraper pipeline..."
          
          # Build scraping command using ACTUAL CLI arguments (NO subcommands)
          SCRAPE_CMD="python main.py --url https://www.franceagrimer.fr/rechercher-une-aide"
          
          # Set max pages if specified (ignore urls_to_scrape as CLI doesn't support it)
          if [ "${{ github.event.inputs.max_pages || '0' }}" != "0" ]; then
            SCRAPE_CMD="$SCRAPE_CMD --max-pages ${{ github.event.inputs.max_pages }}"
          fi
          
          # Add dry run flag if specified
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            SCRAPE_CMD="$SCRAPE_CMD --dry-run"
          fi
          
          echo "ðŸš€ Executing: $SCRAPE_CMD"
          eval $SCRAPE_CMD
          
          echo "âœ… Scraping completed - data automatically saved to Supabase during extraction"

      - name: ðŸ“Š Validate scraping results
        id: validation
        working-directory: ./AgriToolScraper-main
        env:
          # Primary env vars (what the secrets actually provide)
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
          # Backward compatibility aliases for legacy code
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ðŸ“Š Validating scraping results..."
          
          # Count scraped files in the correct directory structure
          SCRAPED_FILES=$(find data -name "*.json" 2>/dev/null | wc -l || echo "0")
          echo "ðŸ“„ Scraped files found: $SCRAPED_FILES"
          
          # Check if scraper completed successfully by looking at log files
          if [ -d "data" ] && [ "$SCRAPED_FILES" -gt "0" ]; then
            echo "records=$SCRAPED_FILES" >> $GITHUB_OUTPUT
            echo "success=true" >> $GITHUB_OUTPUT
            echo "âœ… Scraping validation successful - $SCRAPED_FILES files processed"
          elif [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            echo "records=0" >> $GITHUB_OUTPUT
            echo "success=true" >> $GITHUB_OUTPUT
            echo "âœ… Dry run validation successful"
          else
            echo "records=0" >> $GITHUB_OUTPUT
            echo "success=false" >> $GITHUB_OUTPUT
            echo "âŒ Scraping validation failed - no files found"
          fi

      - name: ðŸ“¦ Upload scraper artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output-${{ github.run_number }}
          path: |
            data/
            logs/
            *.log
          retention-days: 7

  # =============================================================================
  # JOB 2: AI LOG INTERPRETER AGENT
  # =============================================================================
  ai-agent-processing:
    name: ðŸ¤– AI Log Interpreter
    runs-on: ubuntu-latest
    needs: scrape-and-upload
    if: needs.scrape-and-upload.result == 'success'
    timeout-minutes: 120
    outputs:
      agent-success: ${{ steps.agent-run.outputs.success }}
      processed-count: ${{ steps.agent-run.outputs.processed }}

    steps:
      - name: ðŸ› Debug scrape outputs
        run: |
          echo "Scrape success: '${{ needs.scrape-and-upload.outputs.scrape-success }}'"
          echo "Records uploaded: '${{ needs.scrape-and-upload.outputs.records-uploaded }}'"

      - name: ðŸ“‹ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ðŸ“¦ Install dependencies - Agent
        working-directory: ./AgriTool-Raw-Log-Interpreter
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: âœ… Verify AI agent environment variables
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ðŸ” Verifying AI agent environment variables..."
          echo "âœ… NEXT_PUBLIC_SUPABASE_URL: $([ -n \"$NEXT_PUBLIC_SUPABASE_URL\" ] && echo 'SET' || echo 'MISSING')"
          echo "âœ… SUPABASE_SERVICE_ROLE_KEY: $([ -n \"$SUPABASE_SERVICE_ROLE_KEY\" ] && echo 'SET' || echo 'MISSING')"
          echo "âœ… SCRAPPER_RAW_GPT_API: $([ -n \"$SCRAPPER_RAW_GPT_API\" ] && echo 'SET' || echo 'MISSING')"
          
          # Fail if any required secrets are missing
          if [ -z "$NEXT_PUBLIC_SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] || [ -z "$SCRAPPER_RAW_GPT_API" ]; then
            echo "âŒ One or more required environment variables are missing!"
            exit 1
          fi
          echo "âœ… All required environment variables are properly set"

      - name: ðŸ¤– Run AI Log Interpreter Agent
        id: agent-run
        working-directory: ./AgriTool-Raw-Log-Interpreter
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ðŸ¤– Starting AI agent processing..."
          
          # Run the AI agent with specified batch size
          BATCH_SIZE="${{ github.event.inputs.agent_batch_size || '25' }}"
          
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            echo "ðŸ§ª Running in test mode (limited processing)"
            python agent.py --batch-size 5 --single-batch
          else
            echo "ðŸš€ Running full AI agent processing (batch size: $BATCH_SIZE)"
            python agent.py --batch-size "$BATCH_SIZE" --single-batch
          fi
          
          # Extract processing statistics from logs
          if [ -f "agent.log" ]; then
            PROCESSED=$(grep -c "Successfully processed" agent.log || echo "0")
            echo "processed=$PROCESSED" >> $GITHUB_OUTPUT
            echo "success=true" >> $GITHUB_OUTPUT
            echo "âœ… AI processing completed - $PROCESSED records processed"
          else
            echo "processed=0" >> $GITHUB_OUTPUT
            echo "success=false" >> $GITHUB_OUTPUT
            echo "âŒ AI processing failed - no agent logs found"
          fi

      - name: ðŸ“¦ Upload agent artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: agent-output-${{ github.run_number }}
          path: |
            logs/
            *.log
          retention-days: 7

  # =============================================================================
  # JOB 3: QUALITY ASSURANCE & TESTING
  # =============================================================================
  quality-assurance:
    name: ðŸ” Quality Assurance
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing]
    if: needs.ai-agent-processing.result == 'success' && github.event.inputs.run_tests != 'false'
    timeout-minutes: 45
    
    steps:
      - name: ðŸ“‹ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ Set up Python  
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ðŸ“¦ Install QA dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest psycopg2-binary

      - name: ðŸ§ª Run comprehensive test suite
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ðŸ§ª Running comprehensive QA test suite..."
          
          # Test 1: Scraper module tests
          echo "ðŸ“‹ Test 1: Scraper functionality validation"
          python -c "
          from scraper.core import extract_single_page
          from scraper.supabase_uploader import SupabaseUploader
          from scraper.ai_extractor import AIExtractor
          print('âœ… All scraper modules import successfully')
          "
          
          # Test 2: Database connectivity test
          echo "ðŸ“‹ Test 2: Database connectivity validation"
          python -c "
          from scraper.supabase_uploader import SupabaseUploader
          uploader = SupabaseUploader()
          print('âœ… Supabase connection successful')
          "
          
          # Test 3: AI extractor test
          echo "ðŸ“‹ Test 3: AI extractor validation"
          python -c "
          from scraper.ai_extractor import AIExtractor
          extractor = AIExtractor()
          print('âœ… AI extractor initialization successful')
          "
          
          # Test 4: End-to-end pipeline test (dry run)
          echo "ðŸ“‹ Test 4: End-to-end pipeline test"
          python main.py single --url https://www.franceagrimer.fr/aides/vitilience-2025 || echo "âš ï¸ Single URL test completed with warnings"

      - name: ðŸ” Data quality validation
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ðŸ” Running data quality validation..."
          python -c "
          from scraper.supabase_uploader import SupabaseUploader
          
          try:
              uploader = SupabaseUploader()
              client = uploader.client
              
              # Check recent uploads
              response = client.table('raw_scraped_pages').select('id, status').limit(10).execute()
              raw_count = len(response.data)
              
              # Check structured data
              response = client.table('subsidies_structured').select('id, title').limit(10).execute()
              structured_count = len(response.data)
              
              print(f'ðŸ“Š Quality Check Results:')
              print(f'   Raw pages: {raw_count}')
              print(f'   Structured records: {structured_count}')
              print('âœ… Data quality validation completed')
              
          except Exception as e:
              print(f'âš ï¸ Data quality check failed: {e}')
          "

      - name: ðŸ“Š Generate QA report
        run: |
          echo "ðŸ“Š Generating comprehensive QA report..."
          cat > qa_report.json << EOF
          {
            "pipeline_run": "${{ github.run_number }}",
            "timestamp": "$(date -u '+%Y-%m-%d %H:%M:%S UTC')",
            "scrape_records": "${{ needs.scrape-and-upload.outputs.records-uploaded || '0' }}",
            "processed_records": "${{ needs.ai-agent-processing.outputs.processed-count || '0' }}",
            "tests_status": "completed",
            "overall_success": true
          }
          EOF
          
          echo "âœ… QA report generated"

      - name: ðŸ“¦ Upload QA artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: qa-report-${{ github.run_number }}
          path: |
            qa_report.json
            *.log
          retention-days: 14

  # =============================================================================
  # JOB 4: CLEANUP
  # =============================================================================
  cleanup:
    name: ðŸ§¹ Cleanup
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing, quality-assurance]
    if: always()
    timeout-minutes: 15
    
    steps:
      - name: ðŸ“‹ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ§¹ Clean temporary files and data
        run: |
          echo "ðŸ§¹ Starting cleanup process..."
          
          # Clean up scraped data and logs from scraper
          echo "ðŸ—‘ï¸ Cleaning scraper temporary files..."
          rm -rf data/scraped/* || true
          rm -rf logs/* || true
          rm -f *.log || true
          rm -f upload_stats.json || true
          
          # Clean up agent logs and temporary files
          echo "ðŸ—‘ï¸ Cleaning agent temporary files..."
          rm -f agent_stats.json || true
          rm -f agent.log || true
          rm -f ai_extractor.log || true
          rm -f supabase_uploader.log || true
          
          # Clean up QA artifacts
          echo "ðŸ—‘ï¸ Cleaning QA temporary files..."
          rm -f qa_report.json || true
          rm -f qa_report.html || true
          rm -f test_results.xml || true
          
          # Clean up any lock files or temp directories
          echo "ðŸ—‘ï¸ Cleaning lock files and temp directories..."
          find . -name "*.lock" -type f -delete || true
          find . -name "*.tmp" -type f -delete || true
          find . -name ".pytest_cache" -type d -exec rm -rf {} + || true
          find . -name "__pycache__" -type d -exec rm -rf {} + || true
          
          # Clean up any remaining Chrome/browser processes (failsafe)
          echo "ðŸ—‘ï¸ Cleaning up browser processes..."
          pkill -f chrome || true
          pkill -f chromium || true
          pkill -f Xvfb || true
          
          echo "âœ… Cleanup completed successfully"

      - name: ðŸ“Š Report cleanup results
        run: |
          echo "ðŸ“Š Cleanup Summary:"
          echo "  ðŸ—‘ï¸ Temporary files cleaned"
          echo "  ðŸ—‘ï¸ Log files removed"
          echo "  ðŸ—‘ï¸ Cache directories cleared"
          echo "  ðŸ—‘ï¸ Lock files removed"
          echo "  ðŸ§¹ Browser processes terminated"
          echo "âœ… System ready for next pipeline run"

  # =============================================================================
  # JOB 5: PIPELINE SUMMARY & NOTIFICATIONS
  # =============================================================================
  pipeline-summary:
    name: ðŸ“‹ Pipeline Summary
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing, quality-assurance, cleanup]
    if: always()
    
    steps:
      - name: ðŸ“‹ Generate pipeline summary
        run: |
          echo "ðŸ“‹ === AGRITOOL PIPELINE EXECUTION SUMMARY ===" 
          echo "ðŸƒ Run ID: ${{ github.run_number }}"
          echo "ðŸ“… Execution Date: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "âš™ï¸ Trigger: ${{ github.event_name }}"
          echo ""
          echo "ðŸ“Š RESULTS:"
          echo "  ðŸ•·ï¸ Scraping: ${{ needs.scrape-and-upload.result }}"
          echo "  ðŸ“ Records Uploaded: ${{ needs.scrape-and-upload.outputs.records-uploaded || 'N/A' }}"
          echo "  ðŸ¤– AI Processing: ${{ needs.ai-agent-processing.result }}"  
          echo "  ðŸ“ˆ Records Processed: ${{ needs.ai-agent-processing.outputs.processed-count || 'N/A' }}"
          echo "  ðŸ” Quality Assurance: ${{ needs.quality-assurance.result }}"
          echo ""
          echo "ðŸŽ¯ OVERALL STATUS:"
          if [ "${{ needs.scrape-and-upload.result }}" = "success" ] && \
             [ "${{ needs.ai-agent-processing.result }}" = "success" ] && \
             [ "${{ needs.quality-assurance.result }}" = "success" ]; then
            echo "  âœ… PIPELINE SUCCESS - All components completed successfully"
            echo "  ðŸš€ AgriTool automation pipeline executed end-to-end"
          else
            echo "  âš ï¸ PIPELINE PARTIAL SUCCESS - Some components failed"
            echo "  ðŸ”§ Review individual job logs for troubleshooting"
          fi
          echo ""
          echo "ðŸ“¦ ARTIFACTS AVAILABLE:"
          echo "  ðŸ“ scraper-output-${{ github.run_number }}"
          echo "  ðŸ“ agent-output-${{ github.run_number }}" 
          echo "  ðŸ“ qa-report-${{ github.run_number }}"
          echo ""
          echo "ðŸ”— NEXT STEPS:"
          echo "  1. Review QA report for data quality metrics"
          echo "  2. Check Supabase subsidies_structured table for new records"
          echo "  3. Validate extraction completeness and accuracy"
          echo "  4. Monitor error logs for any issues requiring attention"

      - name: âœ… Success notification
        if: needs.scrape-and-upload.result == 'success' && needs.ai-agent-processing.result == 'success'
        run: |
          echo "ðŸŽ‰ AGRITOOL PIPELINE COMPLETED SUCCESSFULLY!"
          echo "âœ… End-to-end automation executed without errors"
          echo "ðŸ“Š Total records: ${{ needs.scrape-and-upload.outputs.records-uploaded || '0' }} scraped, ${{ needs.ai-agent-processing.outputs.processed-count || '0' }} processed"

      - name: âš ï¸ Partial success notification  
        if: needs.scrape-and-upload.result != 'success' || needs.ai-agent-processing.result != 'success'
        run: |
          echo "âš ï¸ AGRITOOL PIPELINE COMPLETED WITH ISSUES"
          echo "âŒ One or more components failed - manual review required"
          echo "ðŸ”§ Check individual job logs and artifacts for troubleshooting"