# =============================================================================
# AgriTool Automated End-to-End Pipeline
# =============================================================================
# Comprehensive automation workflow that handles:
# 1. Data scraping from FranceAgriMer
# 2. Raw data upload to Supabase raw_logs table  
# 3. AI Log Interpreter agent processing
# 4. Final structured data population
# 5. Quality assurance and validation
# =============================================================================

name: üåæ AgriTool Automated Pipeline

on:
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape (0 = unlimited)'
        required: false
        default: '0'
        type: string
      dry_run:
        description: 'Dry run mode (no database writes)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      run_tests:
        description: 'Run comprehensive test suite'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'
      agent_batch_size:
        description: 'AI agent batch size'
        required: false
        default: '25'
        type: string
  schedule:
    # Run daily at 2 AM UTC for full automation
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  DISPLAY: ':99'
  PYTHONUNBUFFERED: '1'

jobs:
  # =============================================================================
  # JOB 1: SCRAPING & RAW DATA UPLOAD
  # =============================================================================
  scrape-and-upload:
    name: üï∑Ô∏è Scrape & Upload Raw Data
    runs-on: ubuntu-latest
    timeout-minutes: 90
    outputs:
      scrape-success: ${{ steps.validation.outputs.success }}
      records-uploaded: ${{ steps.validation.outputs.records }}
      
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üîß Install system dependencies
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y chromium-browser chromium-chromedriver xvfb file tesseract-ocr default-jre

      - name: üñ•Ô∏è Setup virtual display
        run: |
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          sleep 3
        env:
          DISPLAY: ':99'

      - name: üì¶ Install dependencies - Scraper
        working-directory: ./AgriToolScraper-main
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: üï∑Ô∏è Run FranceAgriMer scraper with enhanced extraction
        working-directory: ./AgriToolScraper-main
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DISPLAY: ':99'
        run: |
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            python main.py \
              --url "https://www.franceagrimer.fr/rechercher-une-aide" \
              --max-pages "${{ github.event.inputs.max_pages || '0' }}" \
              --dry-run
          else
            python main.py \
              --url "https://www.franceagrimer.fr/rechercher-une-aide" \
              --max-pages "${{ github.event.inputs.max_pages || '0' }}"
          fi

      - name: üìä Validate scraping results
        id: validation
        working-directory: ./AgriToolScraper-main
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          # Run data pipeline validation test
          python test_data_pipeline_fix.py
          
          # Extract upload statistics
          if [ -f "upload_stats.json" ]; then
            RECORDS=$(cat upload_stats.json | grep -o '"records_uploaded":[0-9]*' | cut -d':' -f2 || echo "0")
            echo "records=$RECORDS" >> $GITHUB_OUTPUT
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Pipeline validation successful - $RECORDS records uploaded"
          else
            echo "success=false" >> $GITHUB_OUTPUT
            echo "records=0" >> $GITHUB_OUTPUT
            echo "‚ùå Pipeline validation failed - no upload stats found"
          fi

      - name: üì¶ Upload scraper artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output-${{ github.run_number }}
          path: |
            AgriToolScraper-main/data/
            AgriToolScraper-main/*.log
            AgriToolScraper-main/upload_stats.json
          retention-days: 7

  # =============================================================================
  # JOB 2: AI LOG INTERPRETER AGENT
  # =============================================================================
  ai-agent-processing:
    name: ü§ñ AI Log Interpreter
    runs-on: ubuntu-latest
    needs: scrape-and-upload
    if: needs.scrape-and-upload.result == 'success'
    timeout-minutes: 120
    outputs:
      agent-success: ${{ steps.agent-run.outputs.success }}
      processed-count: ${{ steps.agent-run.outputs.processed }}

    steps:
      - name: üêõ Debug scrape outputs
        run: |
          echo "Scrape success: '${{ needs.scrape-and-upload.outputs.scrape-success }}'"
          echo "Records uploaded: '${{ needs.scrape-and-upload.outputs.records-uploaded }}'"

      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üîß Install system dependencies for agent
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y tesseract-ocr default-jre

      - name: üì¶ Install dependencies - Agent
        working-directory: ./AgriTool-Raw-Log-Interpreter
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ü§ñ Run AI Log Interpreter Agent
        id: agent-run
        working-directory: ./AgriTool-Raw-Log-Interpreter
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          # Map secret with legacy double-P spelling to expected env variable
          SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "ü§ñ Starting AI agent processing..."

          # Debug environment variables (without exposing values)
          echo "‚úÖ SUPABASE_URL: $([ -n \"$SUPABASE_URL\" ] && echo 'SET' || echo 'MISSING')"
          echo "‚úÖ NEXT_PUBLIC_SUPABASE_ANON: $([ -n \"$NEXT_PUBLIC_SUPABASE_ANON\" ] && echo 'SET' || echo 'MISSING')"
          echo "‚úÖ SUPABASE_SERVICE_ROLE_KEY: $([ -n \"$SUPABASE_SERVICE_ROLE_KEY\" ] && echo 'SET' || echo 'MISSING')"
          echo "‚úÖ SCRAPER_RAW_GPT_API: $([ -n \"$SCRAPER_RAW_GPT_API\" ] && echo 'SET' || echo 'MISSING')"

          # Ensure required secret is present
          if [ -z "$SCRAPER_RAW_GPT_API" ]; then
            echo "SCRAPER_RAW_GPT_API missing!"
            exit 1
          else
            echo "SCRAPER_RAW_GPT_API is set"
          fi
          
          # Run the enhanced agent with specified batch size
          BATCH_SIZE="${{ github.event.inputs.agent_batch_size || '25' }}"
          
          if [ "${{ github.event.inputs.dry_run || 'false' }}" = "true" ]; then
            echo "üß™ Running in test mode (single batch)"
            python agent.py --batch-size 5 --single-batch
          else
            echo "üöÄ Running full agent processing (batch size: $BATCH_SIZE)"
            python agent.py --batch-size "$BATCH_SIZE" --single-batch
          fi
          
          # Extract processing statistics
          if [ -f "agent_stats.json" ]; then
            PROCESSED=$(cat agent_stats.json | grep -o '"processed":[0-9]*' | cut -d':' -f2 || echo "0")
            echo "processed=$PROCESSED" >> $GITHUB_OUTPUT
            echo "success=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Agent processing successful - $PROCESSED records processed"
          else
            # Check agent logs for processing info
            if [ -f "agent.log" ]; then
              PROCESSED=$(grep -c "Successfully processed" agent.log || echo "0")
              echo "processed=$PROCESSED" >> $GITHUB_OUTPUT
              echo "success=true" >> $GITHUB_OUTPUT
              echo "‚úÖ Agent processing completed - $PROCESSED records from logs"
            else
              echo "processed=0" >> $GITHUB_OUTPUT
              echo "success=false" >> $GITHUB_OUTPUT
              echo "‚ùå Agent processing failed - no stats or logs found"
            fi
          fi

      - name: üì¶ Upload agent artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: agent-output-${{ github.run_number }}
          path: |
            AgriTool-Raw-Log-Interpreter/*.log
            AgriTool-Raw-Log-Interpreter/agent_stats.json
          retention-days: 7

  # =============================================================================
  # JOB 3: PDF EXTRACTION PIPELINE
  # =============================================================================
  pdf-extraction:
    name: üìÑ PDF Extraction Pipeline
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing]
    if: needs.ai-agent-processing.result == 'success'
    timeout-minutes: 60
    outputs:
      extraction-success: ${{ steps.pdf-extraction.outputs.success }}
      files-processed: ${{ steps.pdf-extraction.outputs.files-processed }}
      
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üîß Install system dependencies for PDF processing
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y \
            ghostscript \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-fra \
            default-jre \
            poppler-utils \
            ocrmypdf

      - name: üöÄ Start Apache Tika Server
        run: |
          echo "üöÄ Starting Apache Tika Server..."
          wget -q https://downloads.apache.org/tika/2.9.1/tika-server-standard-2.9.1.jar -O tika-server.jar
          java -jar tika-server.jar --host=0.0.0.0 --port=9998 &
          TIKA_PID=$!
          echo "TIKA_PID=$TIKA_PID" >> $GITHUB_ENV
          
          # Wait for Tika to be ready (max 60 seconds)
          echo "‚è≥ Waiting for Tika server to start..."
          for i in {1..60}; do
            if curl -s http://localhost:9998/tika/version > /dev/null 2>&1; then
              echo "‚úÖ Tika server is ready!"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "‚ùå Tika server failed to start within 60 seconds"
              exit 1
            fi
            sleep 1
          done

      - name: üì¶ Install Python dependencies for PDF extraction
        run: |
          python -m pip install --upgrade pip
          pip install requests ocrmypdf

      - name: üìÑ Run PDF Extraction Pipeline
        id: pdf-extraction
        env:
          TIKA_URL: "http://localhost:9998/tika"
          PYTHONPATH: ${{ github.workspace }}/AgriToolScraper-main
        run: |
          echo "üìÑ Starting PDF extraction pipeline..."
          
          # Create subsidies_docs directory if it doesn't exist
          mkdir -p subsidies_docs/samples
          
          # Check if we have any PDF files to process
          PDF_COUNT=$(find subsidies_docs/ -name "*.pdf" -type f | wc -l)
          echo "üìä Found $PDF_COUNT PDF files to process"
          
          if [ $PDF_COUNT -eq 0 ]; then
            echo "‚ö†Ô∏è No PDF files found in subsidies_docs/ - creating sample for demonstration"
            echo "success=true" >> $GITHUB_OUTPUT
            echo "files-processed=0" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Process PDFs using our extraction pipeline
          PROCESSED=0
          FAILED=0
          
          for pdf_file in subsidies_docs/*.pdf subsidies_docs/*/*.pdf; do
            if [ -f "$pdf_file" ]; then
              echo "üîÑ Processing: $pdf_file"
              
              if python AgriToolScraper-main/pdf_extraction_pipeline.py "$pdf_file"; then
                echo "‚úÖ Successfully processed: $pdf_file"
                PROCESSED=$((PROCESSED + 1))
              else
                echo "‚ùå Failed to process: $pdf_file"
                FAILED=$((FAILED + 1))
              fi
            fi
          done
          
          echo "üìä PDF Processing Summary:"
          echo "  ‚úÖ Successfully processed: $PROCESSED files"
          echo "  ‚ùå Failed to process: $FAILED files"
          
          # Set outputs
          echo "files-processed=$PROCESSED" >> $GITHUB_OUTPUT
          
          if [ $FAILED -eq 0 ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "üéâ All PDF files processed successfully!"
          else
            echo "success=partial" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Some PDF files failed to process"
          fi

      - name: üóëÔ∏è Cleanup Tika Server
        if: always()
        run: |
          if [ -n "$TIKA_PID" ]; then
            echo "üóëÔ∏è Stopping Tika server (PID: $TIKA_PID)..."
            kill $TIKA_PID || true
          fi

      - name: üì¶ Upload PDF extraction artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pdf-extraction-output-${{ github.run_number }}
          path: |
            subsidies_docs/
            *.log
          retention-days: 7

  # =============================================================================
  # JOB 4: QUALITY ASSURANCE & TESTING
  # =============================================================================
  quality-assurance:
    name: üîç Quality Assurance
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing]
    if: needs.ai-agent-processing.result == 'success' && github.event.inputs.run_tests != 'false'
    timeout-minutes: 45
    
    steps:
      - name: üìã Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python  
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üì¶ Install QA dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest psycopg2-binary requests supabase tenacity openai tika>=2.6.0

      - name: üß™ Run comprehensive test suite
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          # Map secret with legacy double-P spelling to expected env variable
          SCRAPER_RAW_GPT_API: ${{ secrets.SCRAPPER_RAW_GPT_API }}
        run: |
          echo "üß™ Running comprehensive QA test suite..."
          
          # Test 1: Data pipeline integration test
          echo "üìã Test 1: Data pipeline validation"
          cd AgriToolScraper-main
          python test_data_pipeline_fix.py
          cd ..
          
          # Test 2: Agent robustness test  
          echo "üìã Test 2: Agent robustness validation"
          cd AgriTool-Raw-Log-Interpreter
          python test_robust_agent.py
          cd ..
          
          # Test 3: End-to-end integration test
          echo "üìã Test 3: End-to-end integration test"
          python test_end_to_end_pipeline.py

      - name: üîç Data quality validation
        env:
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          echo "üîç Running data quality validation..."
          python validate_data_quality.py

      - name: üìä Generate QA report
        run: |
          echo "üìä Generating comprehensive QA report..."
          python generate_qa_report.py \
            --scrape-records "${{ needs.scrape-and-upload.outputs.records-uploaded || '0' }}" \
            --processed-records "${{ needs.ai-agent-processing.outputs.processed-count || '0' }}" \
            --run-id "${{ github.run_number }}"

      - name: üì¶ Upload QA artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: qa-report-${{ github.run_number }}
          path: |
            qa_report.html
            qa_report.json
            test_results.xml
          retention-days: 14

  # =============================================================================
  # JOB 4: PIPELINE SUMMARY & NOTIFICATIONS
  # =============================================================================
  pipeline-summary:
    name: üìã Pipeline Summary
    runs-on: ubuntu-latest
    needs: [scrape-and-upload, ai-agent-processing, pdf-extraction, quality-assurance]
    if: always()
    
    steps:
      - name: üìã Generate pipeline summary
        run: |
          echo "üìã === AGRITOOL PIPELINE EXECUTION SUMMARY ===" 
          echo "üèÉ Run ID: ${{ github.run_number }}"
          echo "üìÖ Execution Date: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "‚öôÔ∏è Trigger: ${{ github.event_name }}"
          echo ""
           echo "üìä RESULTS:"
           echo "  üï∑Ô∏è Scraping: ${{ needs.scrape-and-upload.result }}"
           echo "  üìù Records Uploaded: ${{ needs.scrape-and-upload.outputs.records-uploaded || 'N/A' }}"
           echo "  ü§ñ AI Processing: ${{ needs.ai-agent-processing.result }}"  
           echo "  üìà Records Processed: ${{ needs.ai-agent-processing.outputs.processed-count || 'N/A' }}"
           echo "  üìÑ PDF Extraction: ${{ needs.pdf-extraction.result }}"
           echo "  üìã PDFs Processed: ${{ needs.pdf-extraction.outputs.files-processed || 'N/A' }}"
           echo "  üîç Quality Assurance: ${{ needs.quality-assurance.result }}"
          echo ""
          echo "üéØ OVERALL STATUS:"
          if [ "${{ needs.scrape-and-upload.result }}" = "success" ] && \
             [ "${{ needs.ai-agent-processing.result }}" = "success" ] && \
             [ "${{ needs.quality-assurance.result }}" = "success" ]; then
            echo "  ‚úÖ PIPELINE SUCCESS - All components completed successfully"
            echo "  üöÄ AgriTool automation pipeline executed end-to-end"
          else
            echo "  ‚ö†Ô∏è PIPELINE PARTIAL SUCCESS - Some components failed"
            echo "  üîß Review individual job logs for troubleshooting"
          fi
          echo ""
          echo "üì¶ ARTIFACTS AVAILABLE:"
          echo "  üìÅ scraper-output-${{ github.run_number }}"
          echo "  üìÅ agent-output-${{ github.run_number }}" 
          echo "  üìÅ qa-report-${{ github.run_number }}"
          echo ""
          echo "üîó NEXT STEPS:"
          echo "  1. Review QA report for data quality metrics"
          echo "  2. Check Supabase subsidies_structured table for new records"
          echo "  3. Validate extraction completeness and accuracy"
          echo "  4. Monitor error logs for any issues requiring attention"

      - name: ‚úÖ Success notification
        if: needs.scrape-and-upload.result == 'success' && needs.ai-agent-processing.result == 'success'
        run: |
          echo "üéâ AGRITOOL PIPELINE COMPLETED SUCCESSFULLY!"
          echo "‚úÖ End-to-end automation executed without errors"
          echo "üìä Total records: ${{ needs.scrape-and-upload.outputs.records-uploaded || '0' }} scraped, ${{ needs.ai-agent-processing.outputs.processed-count || '0' }} processed"

      - name: ‚ö†Ô∏è Partial success notification  
        if: needs.scrape-and-upload.result != 'success' || needs.ai-agent-processing.result != 'success'
        run: |
          echo "‚ö†Ô∏è AGRITOOL PIPELINE COMPLETED WITH ISSUES"
          echo "‚ùå One or more components failed - manual review required"
          echo "üîß Check individual job logs and artifacts for troubleshooting"